{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saffarizadeh/INSY5378/blob/main/Backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJmXddcuAvqv"
      },
      "source": [
        "<img src=\"https://kambizsaffari.com/Logo/College_of_Business.cmyk-hz-lg.png\" width=\"500px\"/>\n",
        "\n",
        "# *INSY 5378 - Advanced AI*\n",
        "\n",
        "# **Backpropagation with Matrix Calculus**\n",
        "\n",
        "Instructor: Dr. Kambiz Saffari\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gVUWWLdAvqw"
      },
      "source": [
        "## What Are We Trying to Do?\n",
        "\n",
        "A neural network is just a machine that takes in numbers, does some math, and spits out a prediction. The \"math\" involves multiplying your input by a bunch of numbers called **weights**. At first, these weights are random, so the predictions are garbage.\n",
        "\n",
        "**Training** is the process of adjusting these weights so the predictions get better. But how do we know which way to adjust each weight? That's where **backpropagation** comes in.\n",
        "\n",
        "Here's the intuition:\n",
        "\n",
        "1. **We make a prediction** — Feed some data through the network.\n",
        "2. **We measure how wrong we are** — Compare the prediction to the correct answer. The difference is called the **loss**.\n",
        "3. **We figure out who's to blame** — For each weight in the network, we ask: \"If I nudged this weight up a tiny bit, would the loss go up or down? By how much?\" This is the **gradient**.\n",
        "4. **We adjust the weights** — Nudge each weight in the direction that makes the loss smaller.\n",
        "5. **Repeat** — Do this thousands of times until the loss is small and the predictions are good.\n",
        "\n",
        "The tricky part is step 3. A neural network might have millions of weights. We need an efficient way to compute the gradient for *all* of them at once. That's what backpropagation does — it uses the **chain rule** from calculus to propagate the error backward through the network, layer by layer, computing all the gradients in one pass.\n",
        "\n",
        "The math below looks intimidating, but it's just a precise way of answering: **\"How much does each weight contribute to the error, and in which direction?\"**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rBpxKxWAvqw"
      },
      "source": [
        "## The Model We're Using\n",
        "\n",
        "We're using the simplest possible neural network that's still interesting: **two layers**.\n",
        "\n",
        "```\n",
        "Input (n features)\n",
        "    ↓\n",
        "[Layer 1] — multiply by weights, add bias, apply ReLU\n",
        "    ↓\n",
        "Hidden (h neurons)\n",
        "    ↓\n",
        "[Layer 2] — multiply by weights, add bias\n",
        "    ↓\n",
        "Output (m values)\n",
        "```\n",
        "\n",
        "**What's ReLU?** It's a simple function: if the number is negative, make it zero. Otherwise, keep it. That's it. Written as: $\\text{relu}(x) = \\max(0, x)$.\n",
        "\n",
        "**Why ReLU?** Without it, stacking layers would be pointless — two linear transformations in a row is just one linear transformation. ReLU adds non-linearity, which lets the network learn complex patterns.\n",
        "\n",
        "In Keras, this model looks like:\n",
        "\n",
        "```python\n",
        "import keras\n",
        "from keras import layers\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(h, activation='relu', input_shape=(n,)),  # Layer 1: n → h\n",
        "    layers.Dense(m)                                         # Layer 2: h → m\n",
        "])\n",
        "\n",
        "model.compile(optimizer='sgd', loss='mse')\n",
        "model.fit(X, Y, epochs=1000, batch_size=32)\n",
        "```\n",
        "\n",
        "That's it. Keras hides all the backpropagation math from you. But under the hood, it's doing exactly what we derive below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlRxZchVAvqw"
      },
      "source": [
        "## 1. Definitions\n",
        "\n",
        "| Symbol | Shape | Description |\n",
        "|--------|-------|-------------|\n",
        "| $X$ | $B \\times n$ | Input batch ($B$ samples, $n$ features) |\n",
        "| $Y$ | $B \\times m$ | Targets |\n",
        "| $W_1$ | $n \\times h$ | First layer weights |\n",
        "| $b_1$ | $1 \\times h$ | First layer bias |\n",
        "| $W_2$ | $h \\times m$ | Second layer weights |\n",
        "| $b_2$ | $1 \\times m$ | Second layer bias |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg1enzR6Avqw"
      },
      "source": [
        "## 2. Forward Pass\n",
        "\n",
        "$$Z_1 = XW_1 + b_1 \\in \\mathbb{R}^{B \\times h}$$\n",
        "\n",
        "$$A_1 = \\text{relu}(Z_1) \\in \\mathbb{R}^{B \\times h}$$\n",
        "\n",
        "where $\\text{relu}(Z_1)_{ij} = \\max(0, (Z_1)_{ij})$\n",
        "\n",
        "$$\\hat{Y} = A_1 W_2 + b_2 \\in \\mathbb{R}^{B \\times m}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KbHVbiAAvqw"
      },
      "source": [
        "## 3. Loss Function\n",
        "\n",
        "Mean squared error:\n",
        "\n",
        "$$\\ell = \\frac{1}{2B}\\|\\hat{Y} - Y\\|_F^2 \\in \\mathbb{R}$$\n",
        "\n",
        "where $\\|\\cdot\\|_F$ is the Frobenius norm: $\\|M\\|_F^2 = \\sum_{ij} M_{ij}^2$\n",
        "\n",
        "**Why $\\frac{1}{2B}$?**\n",
        "- The $\\frac{1}{B}$ averages over the batch, so the gradient magnitude doesn't depend on batch size.\n",
        "- The $\\frac{1}{2}$ cancels the 2 from differentiating $x^2$, keeping the math cleaner."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf7LeNMrAvqw"
      },
      "source": [
        "## 4. Matrix Calculus Identity\n",
        "\n",
        "Let $A \\in \\mathbb{R}^{p \\times q}$, $B \\in \\mathbb{R}^{q \\times r}$, $Z = AB \\in \\mathbb{R}^{p \\times r}$, and $\\ell = f(Z) \\in \\mathbb{R}$ where $f$ is any differentiable scalar-valued function.\n",
        "\n",
        "Given $\\frac{\\partial \\ell}{\\partial Z} \\in \\mathbb{R}^{p \\times r}$:\n",
        "\n",
        "$$\\frac{\\partial \\ell}{\\partial A} = \\frac{\\partial \\ell}{\\partial Z} B^\\top \\in \\mathbb{R}^{p \\times q}$$\n",
        "\n",
        "$$\\frac{\\partial \\ell}{\\partial B} = A^\\top \\frac{\\partial \\ell}{\\partial Z} \\in \\mathbb{R}^{q \\times r}$$\n",
        "\n",
        "The transpose always goes on the factor you're **not** differentiating with respect to."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGZ2HBE9Avqw"
      },
      "source": [
        "## 5. Backward Pass\n",
        "\n",
        "### 5.1 Gradient of loss: $\\frac{\\partial \\ell}{\\partial W_2}$\n",
        "\n",
        "Chain:\n",
        "$$\\frac{\\partial \\ell}{\\partial W_2} = \\frac{\\partial \\ell}{\\partial \\hat{Y}} \\cdot \\frac{\\partial \\hat{Y}}{\\partial W_2}$$\n",
        "\n",
        "Using the identity $\\frac{\\partial}{\\partial M}\\frac{1}{2}\\|M\\|_F^2 = M$:\n",
        "\n",
        "$$\\frac{\\partial \\ell}{\\partial \\hat{Y}} = \\frac{1}{B}(\\hat{Y} - Y) \\in \\mathbb{R}^{B \\times m}$$\n",
        "\n",
        "Since $\\hat{Y} = A_1 W_2 + b_2$, applying the matrix calculus identity:\n",
        "\n",
        "$$\\boxed{\\frac{\\partial \\ell}{\\partial W_2} = A_1^\\top \\frac{\\partial \\ell}{\\partial \\hat{Y}} = A_1^\\top \\cdot \\frac{1}{B}(\\hat{Y} - Y) \\in \\mathbb{R}^{h \\times m}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oED1oABsAvqw"
      },
      "source": [
        "### 5.2 Gradient of loss: $\\frac{\\partial \\ell}{\\partial b_2}$\n",
        "\n",
        "Chain:\n",
        "$$\\frac{\\partial \\ell}{\\partial b_2} = \\frac{\\partial \\ell}{\\partial \\hat{Y}} \\cdot \\frac{\\partial \\hat{Y}}{\\partial b_2}$$\n",
        "\n",
        "Since $b_2$ is broadcast across all $B$ samples, we sum over the batch:\n",
        "\n",
        "$$\\boxed{\\frac{\\partial \\ell}{\\partial b_2} = \\mathbf{1}^\\top \\frac{\\partial \\ell}{\\partial \\hat{Y}} = \\mathbf{1}^\\top \\cdot \\frac{1}{B}(\\hat{Y} - Y) \\in \\mathbb{R}^{1 \\times m}}$$\n",
        "\n",
        "where $\\mathbf{1} \\in \\mathbb{R}^{B \\times 1}$ is a column vector of ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JekrXeH7Avqw"
      },
      "source": [
        "### 5.3 Gradient of loss: $\\frac{\\partial \\ell}{\\partial W_1}$\n",
        "\n",
        "Chain:\n",
        "$$\\frac{\\partial \\ell}{\\partial W_1} = \\frac{\\partial \\ell}{\\partial \\hat{Y}} \\cdot \\frac{\\partial \\hat{Y}}{\\partial A_1} \\cdot \\frac{\\partial A_1}{\\partial Z_1} \\cdot \\frac{\\partial Z_1}{\\partial W_1}$$\n",
        "\n",
        "Step by step:\n",
        "\n",
        "**Through the second layer ($\\hat{Y} = A_1 W_2 + b_2$):**\n",
        "$$\\frac{\\partial \\ell}{\\partial A_1} = \\frac{\\partial \\ell}{\\partial \\hat{Y}} W_2^\\top = \\frac{1}{B}(\\hat{Y} - Y) W_2^\\top \\in \\mathbb{R}^{B \\times h}$$\n",
        "\n",
        "**Through relu ($A_1 = \\text{relu}(Z_1)$):**\n",
        "$$\\frac{\\partial \\ell}{\\partial Z_1} = \\frac{\\partial \\ell}{\\partial A_1} \\odot \\mathbb{1}[Z_1 > 0] \\in \\mathbb{R}^{B \\times h}$$\n",
        "\n",
        "where $\\odot$ is element-wise multiplication (Hadamard product) and $\\mathbb{1}[Z_1 > 0]$ is 1 where $Z_1 > 0$, else 0.\n",
        "\n",
        "**Through the first layer ($Z_1 = XW_1 + b_1$):**\n",
        "$$\\boxed{\\frac{\\partial \\ell}{\\partial W_1} = X^\\top \\frac{\\partial \\ell}{\\partial Z_1} = X^\\top \\left[\\left(\\frac{1}{B}(\\hat{Y} - Y) W_2^\\top\\right) \\odot \\mathbb{1}[Z_1 > 0]\\right] \\in \\mathbb{R}^{n \\times h}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV64d0_2Avqx"
      },
      "source": [
        "### 5.4 Gradient of loss: $\\frac{\\partial \\ell}{\\partial b_1}$\n",
        "\n",
        "Chain:\n",
        "$$\\frac{\\partial \\ell}{\\partial b_1} = \\frac{\\partial \\ell}{\\partial \\hat{Y}} \\cdot \\frac{\\partial \\hat{Y}}{\\partial A_1} \\cdot \\frac{\\partial A_1}{\\partial Z_1} \\cdot \\frac{\\partial Z_1}{\\partial b_1}$$\n",
        "\n",
        "We already computed $\\frac{\\partial \\ell}{\\partial Z_1}$ above. Since $b_1$ is broadcast across all $B$ samples:\n",
        "\n",
        "$$\\boxed{\\frac{\\partial \\ell}{\\partial b_1} = \\mathbf{1}^\\top \\frac{\\partial \\ell}{\\partial Z_1} = \\mathbf{1}^\\top \\left[\\left(\\frac{1}{B}(\\hat{Y} - Y) W_2^\\top\\right) \\odot \\mathbb{1}[Z_1 > 0]\\right] \\in \\mathbb{R}^{1 \\times h}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JR4dzsiEAvqx"
      },
      "source": [
        "## 6. Summary of Gradients\n",
        "\n",
        "Let $\\delta_2 = \\frac{1}{B}(\\hat{Y} - Y)$ and $\\delta_1 = (\\delta_2 W_2^\\top) \\odot \\mathbb{1}[Z_1 > 0]$. Then:\n",
        "\n",
        "| Gradient | Formula | Shape |\n",
        "|----------|---------|-------|\n",
        "| $\\frac{\\partial \\ell}{\\partial W_2}$ | $A_1^\\top \\delta_2$ | $h \\times m$ |\n",
        "| $\\frac{\\partial \\ell}{\\partial b_2}$ | $\\mathbf{1}^\\top \\delta_2$ | $1 \\times m$ |\n",
        "| $\\frac{\\partial \\ell}{\\partial W_1}$ | $X^\\top \\delta_1$ | $n \\times h$ |\n",
        "| $\\frac{\\partial \\ell}{\\partial b_1}$ | $\\mathbf{1}^\\top \\delta_1$ | $1 \\times h$ |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnOHqKJwAvqx"
      },
      "source": [
        "## 7. Update Rule\n",
        "\n",
        "$$W_2 \\leftarrow W_2 - \\eta \\frac{\\partial \\ell}{\\partial W_2}$$\n",
        "\n",
        "$$b_2 \\leftarrow b_2 - \\eta \\frac{\\partial \\ell}{\\partial b_2}$$\n",
        "\n",
        "$$W_1 \\leftarrow W_1 - \\eta \\frac{\\partial \\ell}{\\partial W_1}$$\n",
        "\n",
        "$$b_1 \\leftarrow b_1 - \\eta \\frac{\\partial \\ell}{\\partial b_1}$$\n",
        "\n",
        "where $\\eta$ is the learning rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Riv3kt2TAvqx"
      },
      "source": [
        "## 8. Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "c56SAGCTAvqx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def relu(Z):\n",
        "    return np.maximum(0, Z)\n",
        "\n",
        "def relu_derivative(Z):\n",
        "    return (Z > 0).astype(float)\n",
        "\n",
        "def forward(X, W1, b1, W2, b2):\n",
        "    Z1 = X @ W1 + b1\n",
        "    A1 = relu(Z1)\n",
        "    Y_hat = A1 @ W2 + b2\n",
        "    return Z1, A1, Y_hat\n",
        "\n",
        "def loss(Y_hat, Y):\n",
        "    B = Y.shape[0]\n",
        "    return (1 / (2 * B)) * np.sum((Y_hat - Y) ** 2)\n",
        "\n",
        "def backward(X, Y, Z1, A1, Y_hat, W2):\n",
        "    B = X.shape[0]\n",
        "\n",
        "    # delta_2 = dL/dY_hat\n",
        "    delta_2 = (1 / B) * (Y_hat - Y)\n",
        "\n",
        "    # Second layer gradients\n",
        "    dW2 = A1.T @ delta_2\n",
        "    db2 = np.sum(delta_2, axis=0, keepdims=True)\n",
        "\n",
        "    # delta_1 = dL/dZ1\n",
        "    delta_1 = (delta_2 @ W2.T) * relu_derivative(Z1)\n",
        "\n",
        "    # First layer gradients\n",
        "    dW1 = X.T @ delta_1\n",
        "    db1 = np.sum(delta_1, axis=0, keepdims=True)\n",
        "\n",
        "    return dW1, db1, dW2, db2\n",
        "\n",
        "def update(W1, b1, W2, b2, dW1, db1, dW2, db2, lr):\n",
        "    W1 = W1 - lr * dW1\n",
        "    b1 = b1 - lr * db1\n",
        "    W2 = W2 - lr * dW2\n",
        "    b2 = b2 - lr * db2\n",
        "    return W1, b1, W2, b2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fKb3TPiAvqx"
      },
      "source": [
        "## 9. Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fxi-XN7yAvqx",
        "outputId": "de53e9a9-ed3b-4366-c351-aabab17a353c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Loss: 2.514020\n",
            "Iteration 100, Loss: 1.454426\n",
            "Iteration 200, Loss: 0.628180\n",
            "Iteration 300, Loss: 0.286071\n",
            "Iteration 400, Loss: 0.104141\n",
            "Iteration 500, Loss: 0.044479\n",
            "Iteration 600, Loss: 0.022456\n",
            "Iteration 700, Loss: 0.012840\n",
            "Iteration 800, Loss: 0.008105\n",
            "Iteration 900, Loss: 0.005215\n"
          ]
        }
      ],
      "source": [
        "# Dimensions\n",
        "B = 32   # batch size\n",
        "n = 10   # input features\n",
        "h = 20   # hidden units\n",
        "m = 5    # output units\n",
        "\n",
        "# Initialize\n",
        "np.random.seed(42)\n",
        "X = np.random.randn(B, n)\n",
        "Y = np.random.randn(B, m)\n",
        "W1 = np.random.randn(n, h) * 0.01\n",
        "b1 = np.zeros((1, h))\n",
        "W2 = np.random.randn(h, m) * 0.01\n",
        "b2 = np.zeros((1, m))\n",
        "\n",
        "# Training loop\n",
        "lr = 0.1\n",
        "for i in range(1000):\n",
        "    Z1, A1, Y_hat = forward(X, W1, b1, W2, b2)\n",
        "    L = loss(Y_hat, Y)\n",
        "    dW1, db1, dW2, db2 = backward(X, Y, Z1, A1, Y_hat, W2)\n",
        "    W1, b1, W2, b2 = update(W1, b1, W2, b2, dW1, db1, dW2, db2, lr)\n",
        "    if i % 100 == 0:\n",
        "        print(f\"Iteration {i}, Loss: {L:.6f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}