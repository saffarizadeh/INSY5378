{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saffarizadeh/INSY5378/blob/main/Math_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzur10HjV_dP"
      },
      "source": [
        "<img src=\"https://kambizsaffari.com/Logo/College_of_Business.cmyk-hz-lg.png\" width=\"500px\"/>\n",
        "\n",
        "# *INSY 5378 - Advanced AI*\n",
        "\n",
        "# **The Mathematical Building Blocks of Neural Networks**\n",
        "\n",
        "Instructor: Dr. Kambiz Saffari\n",
        "\n",
        "---\n",
        "\n",
        "Note: You MUST read the chapter. Going through this notebook does not replace the value of reading the chapter.\n",
        "\n",
        "Link to the chapter: https://deeplearningwithpython.io/chapters/chapter02_mathematical-building-blocks/\n",
        "\n",
        "\n",
        "This chapter covers:\n",
        "- A first example of a neural network\n",
        "- Tensors and tensor operations\n",
        "- How neural networks learn via backpropagation and gradient descent\n",
        "\n",
        "\n",
        "> *Disclaimer: This notebook is a personal study guide created for educational purposes. It summarizes and references material from \"Deep Learning with Python, Third Edition\" by François Chollet and Matthew Watson (Manning Publications). All rights to the original content, including text, images, and code, belong to the respective authors and publisher. This notebook is not intended for commercial use or redistribution. Please support the authors by purchasing the book at Manning.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNCNyx8rV_dR"
      },
      "source": [
        "---\n",
        "## 1. A First Look at a Neural Network\n",
        "\n",
        "Let's build a neural network to classify handwritten digits using the **MNIST dataset** - the \"Hello World\" of deep learning.\n",
        "\n",
        "<img src=\"https://deeplearningwithpython.io/images/ch02/MNIST-sample-digits.3d651e1d.png\" width=\"400\">\n",
        "\n",
        "**Key terminology:**\n",
        "- **Class**: A category in a classification problem\n",
        "- **Sample**: A data point\n",
        "- **Label**: The class associated with a sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzQJ9LfiV_dR"
      },
      "source": [
        "### 1.1 Loading the MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W58noBi3V_dR",
        "outputId": "8959f62c-0e08-4524-920b-45cb05561f1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Training images shape: (60000, 28, 28)\n",
            "Training labels: 60000 samples\n",
            "Sample labels: [5 0 4 1 9 2 1 3 1 4]\n",
            "\n",
            "Test images shape: (10000, 28, 28)\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "print(f\"Training images shape: {train_images.shape}\")\n",
        "print(f\"Training labels: {len(train_labels)} samples\")\n",
        "print(f\"Sample labels: {train_labels[:10]}\")\n",
        "print(f\"\\nTest images shape: {test_images.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsDVLaUnV_dR"
      },
      "source": [
        "### 1.2 Building the Network Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0M7fAg_rV_dS"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(512, activation=\"relu\"),\n",
        "    layers.Dense(10, activation=\"softmax\"),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehJ5NA7oV_dS"
      },
      "source": [
        "**Key concepts:**\n",
        "- **Layer**: A filter for data that extracts representations\n",
        "- **Dense layer**: Fully connected neural layer  \n",
        "- **ReLU activation**: `relu(x) = max(x, 0)`\n",
        "- **Softmax**: Returns probability scores summing to 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBrvzR-HV_dS"
      },
      "source": [
        "### 1.3 Compilation Step\n",
        "\n",
        "Three essential components:\n",
        "1. **Loss function**: Measures performance (what to minimize)\n",
        "2. **Optimizer**: Updates model based on loss\n",
        "3. **Metrics**: What to monitor during training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HsdAc9efV_dS"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHyY1xxxV_dS"
      },
      "source": [
        "### 1.4 Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdQyBbJ5V_dS",
        "outputId": "7509b4e8-b888-4b6b-867c-91b827a4ecb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed shape: (60000, 784)\n",
            "Value range: [0.0, 1.0]\n"
          ]
        }
      ],
      "source": [
        "# Reshape from (60000, 28, 28) to (60000, 784) and normalize to [0, 1]\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype(\"float32\") / 255\n",
        "\n",
        "print(f\"Preprocessed shape: {train_images.shape}\")\n",
        "print(f\"Value range: [{train_images.min()}, {train_images.max()}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfAnFtYzV_dS"
      },
      "source": [
        "### 1.5 Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNqVKN5SV_dS",
        "outputId": "e86efe01-dc52-423b-e1a4-5ba479f1823a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 14ms/step - accuracy: 0.8724 - loss: 0.4590\n",
            "Epoch 2/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9658 - loss: 0.1171\n",
            "Epoch 3/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9808 - loss: 0.0696\n",
            "Epoch 4/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9855 - loss: 0.0498\n",
            "Epoch 5/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9902 - loss: 0.0340\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7bb026fb2690>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "model.fit(train_images, train_labels, epochs=5, batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN97tM2mV_dS"
      },
      "source": [
        "### 1.6 Making Predictions and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "begVA5JqV_dS",
        "outputId": "e9937160-1ed0-4c68-cf53-9b5da994e6f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
            "Prediction for first digit: 7\n",
            "Confidence: 0.999838\n",
            "Actual label: 7\n"
          ]
        }
      ],
      "source": [
        "test_digits = test_images[0:10]\n",
        "predictions = model.predict(test_digits)\n",
        "\n",
        "print(f\"Prediction for first digit: {predictions[0].argmax()}\")\n",
        "print(f\"Confidence: {predictions[0].max():.6f}\")\n",
        "print(f\"Actual label: {test_labels[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVOLXHh-V_dS",
        "outputId": "13ac0c4e-73f2-4f94-d087-1e3caff2529a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9763 - loss: 0.0736\n",
            "Test accuracy: 0.9801\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f\"Test accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peJFzcDqV_dT"
      },
      "source": [
        "**Overfitting**: Test accuracy is lower than training accuracy. Models tend to perform worse on new data than training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0KtnPhQV_dT"
      },
      "source": [
        "---\n",
        "## 2. Data Representations for Neural Networks: Tensors\n",
        "\n",
        "**Tensors** are containers for numerical data - generalizations of matrices to arbitrary dimensions (called **axes**)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYIqWJNZV_dT"
      },
      "source": [
        "### 2.1 Scalars (Rank-0 Tensors)\n",
        "\n",
        "A tensor containing only one number is called a **scalar** (rank-0 tensor)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brEgSSUFV_dT",
        "outputId": "a7de8fcd-b8e8-44cf-8c39-a7a8c59594cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scalar: 12\n",
            "Number of axes (ndim): 0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array(12)\n",
        "print(f\"Scalar: {x}\")\n",
        "print(f\"Number of axes (ndim): {x.ndim}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GD5wqBUsV_dT"
      },
      "source": [
        "### 2.2 Vectors (Rank-1 Tensors)\n",
        "\n",
        "An array of numbers is called a **vector** (rank-1 tensor)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsT5lyxsV_dT",
        "outputId": "06143cd0-9227-4f3a-e667-dd8c3a318e4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector: [12  3  6 14  7]\n",
            "Number of axes (ndim): 1\n",
            "This is a 5-dimensional vector (5 entries along one axis)\n"
          ]
        }
      ],
      "source": [
        "x = np.array([12, 3, 6, 14, 7])\n",
        "print(f\"Vector: {x}\")\n",
        "print(f\"Number of axes (ndim): {x.ndim}\")\n",
        "print(f\"This is a 5-dimensional vector (5 entries along one axis)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIukExOSV_dT"
      },
      "source": [
        "**Important**: Don't confuse \"5D vector\" with \"5D tensor\"!\n",
        "- 5D vector: 1 axis with 5 dimensions along it\n",
        "- 5D tensor: 5 axes (may have any dimensions along each)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtMxPGIOV_dT"
      },
      "source": [
        "### 2.3 Matrices (Rank-2 Tensors)\n",
        "\n",
        "An array of vectors is a **matrix** (rank-2 tensor). It has two axes (rows and columns)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiEyP1EgV_dT",
        "outputId": "e9e32310-4dc8-41a0-ddeb-4f5769cd2eea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix shape: (3, 5)\n",
            "Number of axes (ndim): 2\n",
            "First row: [ 5 78  2 34  0]\n",
            "First column: [5 6 7]\n"
          ]
        }
      ],
      "source": [
        "x = np.array([[5, 78, 2, 34, 0],\n",
        "              [6, 79, 3, 35, 1],\n",
        "              [7, 80, 4, 36, 2]])\n",
        "print(f\"Matrix shape: {x.shape}\")\n",
        "print(f\"Number of axes (ndim): {x.ndim}\")\n",
        "print(f\"First row: {x[0]}\")\n",
        "print(f\"First column: {x[:, 0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0g53-VJZV_dT"
      },
      "source": [
        "### 2.4 Rank-3 and Higher-Rank Tensors\n",
        "\n",
        "Packing matrices in an array gives a **rank-3 tensor** (cube of numbers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2t-U-HlDV_dT",
        "outputId": "b4a0e75d-f666-4127-9d2a-cba19d17cced"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: (3, 3, 5)\n",
            "Number of axes (ndim): 3\n"
          ]
        }
      ],
      "source": [
        "x = np.array([[[5, 78, 2, 34, 0],\n",
        "               [6, 79, 3, 35, 1],\n",
        "               [7, 80, 4, 36, 2]],\n",
        "              [[5, 78, 2, 34, 0],\n",
        "               [6, 79, 3, 35, 1],\n",
        "               [7, 80, 4, 36, 2]],\n",
        "              [[5, 78, 2, 34, 0],\n",
        "               [6, 79, 3, 35, 1],\n",
        "               [7, 80, 4, 36, 2]]])\n",
        "print(f\"Shape: {x.shape}\")\n",
        "print(f\"Number of axes (ndim): {x.ndim}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JE2fETqXV_dT"
      },
      "source": [
        "### 2.5 Key Tensor Attributes\n",
        "\n",
        "1. **Number of axes (rank)**: `ndim` in NumPy\n",
        "2. **Shape**: Tuple of dimensions along each axis\n",
        "3. **Data type (dtype)**: `float32`, `uint8`, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "domhy3m0V_dT",
        "outputId": "ed4e830d-9fd2-4a29-c501-5872873ace0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of axes (ndim): 3\n",
            "Shape: (60000, 28, 28)\n",
            "Data type (dtype): uint8\n",
            "\n",
            "Interpretation: 60000 matrices of 28x28 integers\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import mnist\n",
        "(train_images_raw, _), _ = mnist.load_data()\n",
        "\n",
        "print(f\"Number of axes (ndim): {train_images_raw.ndim}\")\n",
        "print(f\"Shape: {train_images_raw.shape}\")\n",
        "print(f\"Data type (dtype): {train_images_raw.dtype}\")\n",
        "print(f\"\\nInterpretation: {train_images_raw.shape[0]} matrices of {train_images_raw.shape[1]}x{train_images_raw.shape[2]} integers\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot1Q2bXlV_dT"
      },
      "source": [
        "### 2.6 Visualizing a Tensor Element"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "nF1hI2-iV_dT",
        "outputId": "9f963a8a-d58f-418f-f3e8-bb8df3505f65"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist\n",
        "(train_images_raw, train_labels_raw), _ = mnist.load_data()\n",
        "\n",
        "digit = train_images_raw[4]\n",
        "plt.imshow(digit, cmap=plt.cm.binary)\n",
        "plt.title(f\"Label: {train_labels_raw[4]}\")\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kr5HY6VdV_dT"
      },
      "source": [
        "<img src=\"https://deeplearningwithpython.io/images/ch02/The-fourth-sample-in-our-dataset.8685ed9a.png\" width=\"250\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udzVvfIFV_dU"
      },
      "source": [
        "### 2.7 Tensor Slicing\n",
        "\n",
        "Selecting specific elements in a tensor is called **tensor slicing**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSN5B0R5V_dU",
        "outputId": "7cdcb963-590d-4968-8618-f81924f4ae50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slice [10:100] shape: (90, 28, 28)\n",
            "Same slice: (90, 28, 28)\n",
            "Bottom-right corner: (60000, 14, 14)\n",
            "Center 14x14 crop: (60000, 14, 14)\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import mnist\n",
        "(train_images_raw, _), _ = mnist.load_data()\n",
        "\n",
        "# Select digits 10 to 99 (100 not included)\n",
        "my_slice = train_images_raw[10:100]\n",
        "print(f\"Slice [10:100] shape: {my_slice.shape}\")\n",
        "\n",
        "# Equivalent notations\n",
        "my_slice = train_images_raw[10:100, :, :]  # Explicit colons\n",
        "my_slice = train_images_raw[10:100, 0:28, 0:28]  # Fully explicit\n",
        "print(f\"Same slice: {my_slice.shape}\")\n",
        "\n",
        "# Bottom-right 14x14 pixels of all images\n",
        "bottom_right = train_images_raw[:, 14:, 14:]\n",
        "print(f\"Bottom-right corner: {bottom_right.shape}\")\n",
        "\n",
        "# Center crop using negative indices\n",
        "center_crop = train_images_raw[:, 7:-7, 7:-7]\n",
        "print(f\"Center 14x14 crop: {center_crop.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEdLG4CZV_dU"
      },
      "source": [
        "### 2.8 Data Batches\n",
        "\n",
        "Deep learning models process data in **mini-batches**. The first axis (axis 0) is the **batch axis**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjkrtB82V_dU",
        "outputId": "ad7e6730-8558-4a2c-8e0d-887cc2daaf17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First batch: (128, 28, 28)\n",
            "Second batch: (128, 28, 28)\n",
            "Batch 3: (128, 28, 28)\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import mnist\n",
        "(train_images_raw, _), _ = mnist.load_data()\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "# First batch\n",
        "batch_0 = train_images_raw[:batch_size]\n",
        "print(f\"First batch: {batch_0.shape}\")\n",
        "\n",
        "# Second batch\n",
        "batch_1 = train_images_raw[batch_size:2*batch_size]\n",
        "print(f\"Second batch: {batch_1.shape}\")\n",
        "\n",
        "# Nth batch (general formula)\n",
        "n = 3\n",
        "batch_n = train_images_raw[batch_size * n : batch_size * (n + 1)]\n",
        "print(f\"Batch {n}: {batch_n.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cT7srdTjV_dU"
      },
      "source": [
        "### 2.9 Real-World Data Tensor Examples\n",
        "\n",
        "| Data Type | Rank | Shape Format | Example |\n",
        "|-----------|------|--------------|---------|\n",
        "| Vector data | 2 | `(samples, features)` | 100K people × 3 attributes |\n",
        "| Timeseries | 3 | `(samples, timesteps, features)` | 250 days × 390 min × 3 prices |\n",
        "| Images | 4 | `(samples, height, width, channels)` | 128 images × 256 × 256 × 3 |\n",
        "| Video | 5 | `(samples, frames, height, width, channels)` | 4 videos × 240 frames × 144 × 256 × 3 |\n",
        "\n",
        "<img src=\"https://deeplearningwithpython.io/images/ch02/timeseries_data.a711cc5a.png\" width=\"400\">\n",
        "\n",
        "<img src=\"https://deeplearningwithpython.io/images/ch02/image_data.8accee38.png\" width=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMTf-arEV_dU"
      },
      "source": [
        "---\n",
        "## 3. The Gears of Neural Networks: Tensor Operations\n",
        "\n",
        "A Dense layer performs:\n",
        "```python\n",
        "output = relu(matmul(input, W) + b)\n",
        "```\n",
        "\n",
        "Three operations:\n",
        "1. **Tensor product** (`matmul`) between input and weights `W`\n",
        "2. **Addition** of result and bias `b`\n",
        "3. **ReLU**: `relu(x) = max(x, 0)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcqUIf3-V_dU"
      },
      "source": [
        "### 3.1 Element-wise Operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ESA-2zipV_dV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def naive_relu(x):\n",
        "    \"\"\"Naive element-wise relu for rank-2 tensors\"\"\"\n",
        "    assert len(x.shape) == 2\n",
        "    x = x.copy()\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(x.shape[1]):\n",
        "            x[i, j] = max(x[i, j], 0)\n",
        "    return x\n",
        "\n",
        "def naive_add(x, y):\n",
        "    \"\"\"Naive element-wise addition for rank-2 tensors\"\"\"\n",
        "    assert len(x.shape) == 2\n",
        "    assert x.shape == y.shape\n",
        "    x = x.copy()\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(x.shape[1]):\n",
        "            x[i, j] += y[i, j]\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_HER-QZ6V_dV"
      },
      "outputs": [],
      "source": [
        "# NumPy's optimized versions\n",
        "import numpy as np\n",
        "\n",
        "x = np.random.random((20, 100))\n",
        "y = np.random.random((20, 100))\n",
        "\n",
        "z = x + y              # Element-wise addition\n",
        "z = np.maximum(z, 0.)  # Element-wise relu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnewAfo2V_dV",
        "outputId": "f539a59b-2d6a-45c7-ee26-69417ae7460e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy vectorized: 0.01s\n",
            "Naive Python loops: 2.01s\n"
          ]
        }
      ],
      "source": [
        "# Speed comparison\n",
        "import time\n",
        "\n",
        "x = np.random.random((20, 100))\n",
        "y = np.random.random((20, 100))\n",
        "\n",
        "t0 = time.time()\n",
        "for _ in range(1000):\n",
        "    z = x + y\n",
        "    z = np.maximum(z, 0.)\n",
        "print(f\"NumPy vectorized: {time.time() - t0:.2f}s\")\n",
        "\n",
        "t0 = time.time()\n",
        "for _ in range(1000):\n",
        "    z = naive_add(x, y)\n",
        "    z = naive_relu(z)\n",
        "print(f\"Naive Python loops: {time.time() - t0:.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_spBTjRV_dV"
      },
      "source": [
        "### 3.2 Broadcasting\n",
        "\n",
        "When tensor shapes differ, the smaller tensor is **broadcast** to match the larger one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XmH6zbjV_dV",
        "outputId": "18218c99-a95a-49eb-9a0e-6149d230efd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y expanded: (1, 10)\n",
            "Y tiled: (32, 10)\n",
            "X + y (broadcast): (32, 10)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# X: (32, 10), y: (10,)\n",
        "X = np.random.random((32, 10))\n",
        "y = np.random.random((10,))\n",
        "\n",
        "# Conceptually, broadcasting does:\n",
        "# 1. Add axis: (10,) -> (1, 10)\n",
        "y_expanded = np.expand_dims(y, axis=0)\n",
        "print(f\"y expanded: {y_expanded.shape}\")\n",
        "\n",
        "# 2. Repeat along new axis: (1, 10) -> (32, 10)\n",
        "Y = np.tile(y_expanded, (32, 1))\n",
        "print(f\"Y tiled: {Y.shape}\")\n",
        "\n",
        "# But NumPy does this automatically!\n",
        "result = X + y\n",
        "print(f\"X + y (broadcast): {result.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lLrz7jnV_dV",
        "outputId": "4ea50bfe-8a04-461d-aa3b-dc88a874a21c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive broadcast result: (32, 10)\n"
          ]
        }
      ],
      "source": [
        "def naive_add_matrix_and_vector(x, y):\n",
        "    \"\"\"Naive broadcasting implementation\"\"\"\n",
        "    assert len(x.shape) == 2\n",
        "    assert len(y.shape) == 1\n",
        "    assert x.shape[1] == y.shape[0]\n",
        "    x = x.copy()\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(x.shape[1]):\n",
        "            x[i, j] += y[j]\n",
        "    return x\n",
        "\n",
        "result = naive_add_matrix_and_vector(X, y)\n",
        "print(f\"Naive broadcast result: {result.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBa9hV1mV_dV",
        "outputId": "14365b27-ce03-4b90-a35c-a301f2563556"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result shape: (64, 3, 32, 10)\n"
          ]
        }
      ],
      "source": [
        "# Broadcasting with higher-rank tensors\n",
        "x = np.random.random((64, 3, 32, 10))\n",
        "y = np.random.random((32, 10))\n",
        "z = np.maximum(x, y)  # y broadcasts to (64, 3, 32, 10)\n",
        "print(f\"Result shape: {z.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4wq-RuoV_dV"
      },
      "source": [
        "### 3.3 Tensor Product (Matrix Multiplication)\n",
        "\n",
        "The **tensor product** (dot product, matmul) is the most important operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHOA3QfrV_dV",
        "outputId": "45d5a343-3519-4997-a065-be888ba75bc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive: 32.0\n",
            "NumPy: 32.0\n",
            "Using @: 32.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def naive_vector_dot(x, y):\n",
        "    \"\"\"Dot product of two vectors -> scalar\"\"\"\n",
        "    assert len(x.shape) == 1\n",
        "    assert len(y.shape) == 1\n",
        "    assert x.shape[0] == y.shape[0]\n",
        "    z = 0.\n",
        "    for i in range(x.shape[0]):\n",
        "        z += x[i] * y[i]\n",
        "    return z\n",
        "\n",
        "x = np.array([1., 2., 3.])\n",
        "y = np.array([4., 5., 6.])\n",
        "print(f\"Naive: {naive_vector_dot(x, y)}\")\n",
        "print(f\"NumPy: {np.dot(x, y)}\")\n",
        "print(f\"Using @: {x @ y}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UHxuHpKV_dV",
        "outputId": "425d5405-6b6e-457e-d773-eb2713851796"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix-vector product: [23. 53. 83.]\n",
            "Using matmul: [23. 53. 83.]\n"
          ]
        }
      ],
      "source": [
        "def naive_matrix_vector_dot(x, y):\n",
        "    \"\"\"Matrix-vector product -> vector\"\"\"\n",
        "    assert len(x.shape) == 2\n",
        "    assert len(y.shape) == 1\n",
        "    assert x.shape[1] == y.shape[0]\n",
        "    z = np.zeros(x.shape[0])\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(x.shape[1]):\n",
        "            z[i] += x[i, j] * y[j]\n",
        "    return z\n",
        "\n",
        "x = np.array([[1., 2.], [3., 4.], [5., 6.]])  # 3x2\n",
        "y = np.array([7., 8.])  # 2\n",
        "print(f\"Matrix-vector product: {naive_matrix_vector_dot(x, y)}\")\n",
        "print(f\"Using matmul: {np.matmul(x, y)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zxqqso_V_dV",
        "outputId": "5094b66b-38a3-4713-e14f-b964f98d9ece"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3,4) @ (4,5) = (3, 5)\n"
          ]
        }
      ],
      "source": [
        "def naive_matrix_dot(x, y):\n",
        "    \"\"\"Matrix-matrix product\"\"\"\n",
        "    assert len(x.shape) == 2\n",
        "    assert len(y.shape) == 2\n",
        "    assert x.shape[1] == y.shape[0]\n",
        "    z = np.zeros((x.shape[0], y.shape[1]))\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(y.shape[1]):\n",
        "            row_x = x[i, :]\n",
        "            column_y = y[:, j]\n",
        "            z[i, j] = naive_vector_dot(row_x, column_y)\n",
        "    return z\n",
        "\n",
        "x = np.random.random((3, 4))  # 3x4\n",
        "y = np.random.random((4, 5))  # 4x5\n",
        "z = naive_matrix_dot(x, y)\n",
        "print(f\"(3,4) @ (4,5) = {z.shape}\")  # Should be (3,5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOqf63FUV_dV"
      },
      "source": [
        "**Shape compatibility**: `(a, b) @ (b, c) -> (a, c)`\n",
        "\n",
        "<img src=\"https://deeplearningwithpython.io/images/ch02/matrix_dot_box_diagram.3dc0f796.png\" width=\"350\">\n",
        "\n",
        "The width of x must equal the height of y. **Note:** `matmul(x, y) != matmul(y, x)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQxSuZ0zV_dW"
      },
      "source": [
        "### 3.4 Tensor Reshaping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scGXMvkWV_dW",
        "outputId": "89081fe0-9d18-4966-b727-d8d079007d87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: (3, 2)\n",
            "[[0. 1.]\n",
            " [2. 3.]\n",
            " [4. 5.]]\n",
            "\n",
            "Reshaped to (6, 1):\n",
            "[[0.]\n",
            " [1.]\n",
            " [2.]\n",
            " [3.]\n",
            " [4.]\n",
            " [5.]]\n",
            "\n",
            "Reshaped to (2, 3):\n",
            "[[0. 1. 2.]\n",
            " [3. 4. 5.]]\n",
            "\n",
            "Transpose: (300, 20) -> (20, 300)\n"
          ]
        }
      ],
      "source": [
        "x = np.array([[0., 1.],\n",
        "              [2., 3.],\n",
        "              [4., 5.]])\n",
        "print(f\"Original: {x.shape}\")\n",
        "print(x)\n",
        "\n",
        "print(f\"\\nReshaped to (6, 1):\")\n",
        "print(x.reshape((6, 1)))\n",
        "\n",
        "print(f\"\\nReshaped to (2, 3):\")\n",
        "print(x.reshape((2, 3)))\n",
        "\n",
        "# Transposition\n",
        "x = np.zeros((300, 20))\n",
        "print(f\"\\nTranspose: {x.shape} -> {np.transpose(x).shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXGJlebeV_dW"
      },
      "source": [
        "### 3.5 Geometric Interpretation\n",
        "\n",
        "Tensor operations are **geometric transformations**:\n",
        "\n",
        "| Operation | Transformation |\n",
        "|-----------|----------------|\n",
        "| Vector addition | Translation |\n",
        "| Matrix product | Rotation, scaling, linear transform |\n",
        "| Affine (Wx + b) | Linear transform + translation |\n",
        "\n",
        "<img src=\"https://deeplearningwithpython.io/images/ch02/translation.c123da84.png\" width=\"450\" alt=\"2D translation as vector addition - moving an object without distortion\"><br>\n",
        "\n",
        "<img src=\"https://deeplearningwithpython.io/images/ch02/rotation.8f4da7c4.png\" width=\"450\" alt=\"2D rotation as a matrix product - rotating an object around the origin\"><br>\n",
        "\n",
        "<img src=\"https://deeplearningwithpython.io/images/ch02/scaling.8cca5e17.png\" width=\"450\" alt=\"2D scaling as a matrix product - stretching or compressing an object\"><br>\n",
        "\n",
        "<img src=\"https://deeplearningwithpython.io/images/ch02/affine_transform.80be4403.png\" width=\"450\" alt=\"Affine transform combining linear transformation and translation\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prtP_9ogV_dW"
      },
      "source": [
        "### 3.6 Why We Need Activation Functions\n",
        "\n",
        "**Key insight:** Multiple affine transforms = single affine transform!\n",
        "```\n",
        "affine2(affine1(x)) = (W2 @ W1) @ x + (W2 @ b1 + b2)\n",
        "```\n",
        "Without activations, deep networks = linear models!\n",
        "\n",
        "**ReLU breaks linearity**, enabling complex nonlinear transformations:\n",
        "\n",
        "<img src=\"https://deeplearningwithpython.io/images/ch02/function.4b000cb3.png\" width=\"450\" alt=\"A continuous smooth function f(x) = y plotted as a curve\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgbQP9pqV_dW"
      },
      "source": [
        "### 3.7 Deep Learning as Geometric Unfolding\n",
        "\n",
        "Neural networks untangle complex data manifolds through simple transformations.\n",
        "\n",
        "<img src=\"https://deeplearningwithpython.io/images/ch02/geometric_interpretation_4.f8123b83.png\" width=\"700\" alt=\"Uncrumpling a complicated data manifold - neural networks separating tangled data\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAcw3wvxV_dW"
      },
      "source": [
        "---\n",
        "## 4. The Engine of Neural Networks: Gradient-Based Optimization\n",
        "\n",
        "Each layer: `output = relu(matmul(input, W) + b)`\n",
        "\n",
        "`W` and `b` are **trainable parameters** (weights) - initially random, gradually adjusted through training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIZvSxUrV_dW"
      },
      "source": [
        "### 4.1 The Training Loop\n",
        "\n",
        "Repeat until loss is sufficiently low:\n",
        "1. **Draw** a batch of training samples `x` and targets `y_true`\n",
        "2. **Forward pass**: Run model on `x` to get predictions `y_pred`\n",
        "3. **Compute loss**: Measure mismatch between `y_pred` and `y_true`\n",
        "4. **Update weights**: Adjust all weights to reduce the loss\n",
        "\n",
        "Step 4 is the challenge: How do we know which direction to adjust each weight?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXdBr5MjV_dW"
      },
      "source": [
        "### 4.2 The Naive Approach (Don't Do This!)\n",
        "\n",
        "For each weight:\n",
        "1. Freeze all other weights\n",
        "2. Try different values\n",
        "3. Keep the one that reduces loss\n",
        "\n",
        "**Problem:** Requires 2 forward passes per coefficient. With millions of parameters, this is impossibly slow!\n",
        "\n",
        "**Solution:** Use **gradient descent** - leverage that all operations are **differentiable**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BKU8vGCV_dW"
      },
      "source": [
        "### 4.3 What's a Derivative?\n",
        "\n",
        "Consider a continuous, smooth function `f(x) = y`:\n",
        "\n",
        "<img src=\"https://deeplearningwithpython.io/images/ch02/function.4b000cb3.png\" width=\"400\" alt=\"A continuous smooth function f(x) = y plotted as a curve\">\n",
        "\n",
        "Because the function is **continuous**, small changes in `x` cause small changes in `y`:\n",
        "\n",
        "<img src=\"https://deeplearningwithpython.io/images/ch02/continuity.98fd80b7.png\" width=\"400\" alt=\"Continuity - a small change in x results in a small change in y\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOwRnhj6V_dW"
      },
      "source": [
        "Because the function is **smooth**, around point `p` we can approximate `f` linearly:\n",
        "```\n",
        "f(x + epsilon_x) ≈ y + a * epsilon_x\n",
        "```\n",
        "\n",
        "The slope `a` is the **derivative** of `f` at point `p`.\n",
        "\n",
        "<img src=\"https://deeplearningwithpython.io/images/ch02/derivation.306de198.png\" width=\"400\" alt=\"Derivative of f at point p - the slope of the local linear approximation\">\n",
        "\n",
        "**What the derivative tells us:**\n",
        "- `a < 0`: increasing `x` **decreases** `f(x)`\n",
        "- `a > 0`: increasing `x` **increases** `f(x)`\n",
        "- `|a|` tells us how quickly `f(x)` changes\n",
        "\n",
        "**Key insight for optimization:** To **minimize** `f(x)`, move `x` **opposite to the derivative**!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b3UgpgPV_dW",
        "outputId": "c51efa27-8935-475e-d1f2-c363ec902f69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f(x) = x² at x=3.0: f(3.0) = 9.0\n",
            "Numerical derivative: 6.0000\n",
            "Analytical derivative (2x): 6.0\n",
            "\n",
            "Gradient step: 3.0 -> 2.3999999912119847\n",
            "Loss decreased: 9.0 -> 5.759999957817526\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def f(x):\n",
        "    return x ** 2  # Simple parabola, minimum at x=0\n",
        "\n",
        "def numerical_derivative(f, x, epsilon=1e-7):\n",
        "    \"\"\"Approximate derivative using finite differences\"\"\"\n",
        "    return (f(x + epsilon) - f(x)) / epsilon\n",
        "\n",
        "# The derivative of x^2 is 2x\n",
        "x = 3.0\n",
        "print(f\"f(x) = x² at x={x}: f({x}) = {f(x)}\")\n",
        "print(f\"Numerical derivative: {numerical_derivative(f, x):.4f}\")\n",
        "print(f\"Analytical derivative (2x): {2*x}\")\n",
        "\n",
        "# To minimize f(x), move opposite to derivative\n",
        "learning_rate = 0.1\n",
        "derivative = numerical_derivative(f, x)\n",
        "x_new = x - learning_rate * derivative\n",
        "\n",
        "print(f\"\\nGradient step: {x} -> {x_new}\")\n",
        "print(f\"Loss decreased: {f(x)} -> {f(x_new)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvBpbgK7V_dW"
      },
      "source": [
        "### 4.4 Derivative of Tensor Operations: The Gradient\n",
        "\n",
        "**Gradient** = generalization of derivatives to functions with tensor inputs.\n",
        "\n",
        "For a scalar function, the derivative = slope of the curve.\n",
        "For a tensor function, the gradient = curvature of the multidimensional surface."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RkehcAzV_dX"
      },
      "source": [
        "**In machine learning:**\n",
        "```python\n",
        "x        # Input (a sample)\n",
        "W        # Weights (parameters)\n",
        "y_true   # Target\n",
        "loss     # Loss function\n",
        "\n",
        "y_pred = matmul(x, W)\n",
        "loss_value = loss(y_pred, y_true)\n",
        "\n",
        "# grad(loss_value, W) is a tensor same shape as W\n",
        "# Each element grad(loss_value, W)[i,j] tells us:\n",
        "# \"How does loss change when we modify W[i,j]?\"\n",
        "```\n",
        "\n",
        "**Partial derivatives:** The gradient combines partial derivatives for each element of W.\n",
        "\n",
        "**Update rule:** Move `W` opposite to gradient:\n",
        "```python\n",
        "W_new = W - step * grad(loss, W)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vtlh5VSVV_dX",
        "outputId": "44f646a5-3fa3-42f1-85e6-d4abf5f9efad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start: W = [3. 4.], f(W) = 25.0\n",
            "Gradient: [6. 8.]\n",
            "\n",
            "After step: W = [2.4 3.2]\n",
            "f(W) decreased: 25.0 -> 16.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def f(W):\n",
        "    \"\"\"f(W) = W[0]^2 + W[1]^2 (bowl-shaped surface, minimum at origin)\"\"\"\n",
        "    return W[0]**2 + W[1]**2\n",
        "\n",
        "def gradient_f(W):\n",
        "    \"\"\"Gradient: [2*W[0], 2*W[1]]\"\"\"\n",
        "    return np.array([2*W[0], 2*W[1]])\n",
        "\n",
        "# Start somewhere\n",
        "W = np.array([3.0, 4.0])\n",
        "print(f\"Start: W = {W}, f(W) = {f(W)}\")\n",
        "print(f\"Gradient: {gradient_f(W)}\")\n",
        "\n",
        "# Gradient descent step\n",
        "learning_rate = 0.1\n",
        "W_new = W - learning_rate * gradient_f(W)\n",
        "print(f\"\\nAfter step: W = {W_new}\")\n",
        "print(f\"f(W) decreased: {f(W)} -> {f(W_new)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxXkYTuYV_dX",
        "outputId": "3585e5b6-6b78-462c-d5e7-33da1ff43348"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Descent Progress:\n",
            "Step 0: W = [3. 4.], f(W) = 25.0000\n",
            "Step 1: W = [2.4000, 3.2000], f(W) = 16.0000\n",
            "Step 2: W = [1.9200, 2.5600], f(W) = 10.2400\n",
            "Step 3: W = [1.5360, 2.0480], f(W) = 6.5536\n",
            "Step 4: W = [1.2288, 1.6384], f(W) = 4.1943\n",
            "Step 5: W = [0.9830, 1.3107], f(W) = 2.6844\n",
            "Step 6: W = [0.7864, 1.0486], f(W) = 1.7180\n",
            "Step 7: W = [0.6291, 0.8389], f(W) = 1.0995\n",
            "Step 8: W = [0.5033, 0.6711], f(W) = 0.7037\n",
            "Step 9: W = [0.4027, 0.5369], f(W) = 0.4504\n",
            "Step 10: W = [0.3221, 0.4295], f(W) = 0.2882\n",
            "\n",
            "Minimum at W = [0, 0] with f(W) = 0\n"
          ]
        }
      ],
      "source": [
        "# Multiple gradient descent steps\n",
        "W = np.array([3.0, 4.0])\n",
        "learning_rate = 0.1\n",
        "\n",
        "print(\"Gradient Descent Progress:\")\n",
        "print(f\"Step 0: W = {W}, f(W) = {f(W):.4f}\")\n",
        "\n",
        "for step in range(1, 11):\n",
        "    W = W - learning_rate * gradient_f(W)\n",
        "    print(f\"Step {step}: W = [{W[0]:.4f}, {W[1]:.4f}], f(W) = {f(W):.4f}\")\n",
        "\n",
        "print(f\"\\nMinimum at W = [0, 0] with f(W) = 0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sh6pXna_V_dX"
      },
      "source": [
        "### 4.5 Stochastic Gradient Descent (SGD)\n",
        "\n",
        "The algorithm:\n",
        "1. Draw a batch of samples `x` and targets `y_true`\n",
        "2. Forward pass: get predictions `y_pred`\n",
        "3. Compute loss\n",
        "4. **Backward pass**: Compute gradient of loss w.r.t. weights\n",
        "5. Update: `W -= learning_rate * gradient`\n",
        "\n",
        "**Stochastic** = random batches.\n",
        "\n",
        "<img src=\"https://deeplearningwithpython.io/images/ch02/sgd_explained_1.0535e152.png\" width=\"400\" alt=\"Stochastic gradient descent down a 1D loss curve with one learnable parameter\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45Lwpu9-V_dX"
      },
      "source": [
        "**Learning rate matters:**\n",
        "- Too small: Slow convergence, stuck in local minima\n",
        "- Too large: Overshoots, bounces randomly\n",
        "\n",
        "**SGD Variants:**\n",
        "- True SGD: 1 sample at a time\n",
        "- Mini-batch SGD: Small batches (typical)\n",
        "- Batch gradient descent: All data at once\n",
        "\n",
        "<img src=\"https://deeplearningwithpython.io/images/ch02/gradient_descent_3d.85d77c73.png\" width=\"500\" alt=\"Gradient descent down a 2D loss surface with two learnable parameters\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDew924dV_dX"
      },
      "source": [
        "### 4.6 SGD with Momentum\n",
        "\n",
        "**Problem:** SGD can get stuck in local minima.\n",
        "\n",
        "<img src=\"https://deeplearningwithpython.io/images/ch02/global_minimum.8f000c0a.png\" width=\"500\" alt=\"Local minimum versus global minimum on a loss curve\">\n",
        "\n",
        "**Solution: Momentum** - consider past velocity, like a ball rolling downhill."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPlCa36VV_dX",
        "outputId": "7e42fdc3-a277-4dc2-9ced-4a2adbf39698"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SGD with Momentum (minimizing f(w) = w²):\n",
            "Step 1: w = 4.9000, velocity = -0.1000\n",
            "Step 2: w = 4.7120, velocity = -0.1880\n",
            "Step 3: w = 4.4486, velocity = -0.2634\n",
            "Step 4: w = 4.1225, velocity = -0.3261\n",
            "Step 5: w = 3.7466, velocity = -0.3759\n",
            "Step 6: w = 3.3333, velocity = -0.4133\n",
            "Step 7: w = 2.8947, velocity = -0.4386\n",
            "Step 8: w = 2.4421, velocity = -0.4526\n",
            "Step 9: w = 1.9859, velocity = -0.4562\n",
            "Step 10: w = 1.5356, velocity = -0.4503\n"
          ]
        }
      ],
      "source": [
        "def sgd_with_momentum(w, gradient, velocity, learning_rate=0.01, momentum=0.9):\n",
        "    \"\"\"\n",
        "    Update weights using momentum.\n",
        "    Velocity accumulates past gradients for faster, smoother convergence.\n",
        "    \"\"\"\n",
        "    velocity = momentum * velocity - learning_rate * gradient\n",
        "    w = w + velocity\n",
        "    return w, velocity\n",
        "\n",
        "# Demo\n",
        "w = 5.0\n",
        "velocity = 0.0\n",
        "\n",
        "print(\"SGD with Momentum (minimizing f(w) = w²):\")\n",
        "for step in range(10):\n",
        "    gradient = 2 * w  # d/dw(w²) = 2w\n",
        "    w, velocity = sgd_with_momentum(w, gradient, velocity)\n",
        "    print(f\"Step {step+1}: w = {w:.4f}, velocity = {velocity:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYq4ghWKV_dX"
      },
      "source": [
        "### 4.7 The Chain Rule\n",
        "\n",
        "For composed functions `fg(x) = f(g(x))`:\n",
        "```python\n",
        "grad(y, x) = grad(y, x1) * grad(x1, x)\n",
        "```\n",
        "where `x1 = g(x)` and `y = f(x1)`.\n",
        "\n",
        "For longer chains:\n",
        "```python\n",
        "def fghj(x):\n",
        "    x1 = j(x)\n",
        "    x2 = h(x1)\n",
        "    x3 = g(x2)\n",
        "    y = f(x3)\n",
        "    return y\n",
        "\n",
        "grad(y, x) = grad(y, x3) * grad(x3, x2) * grad(x2, x1) * grad(x1, x)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPFKwOVdV_dX",
        "outputId": "3aed33e7-1616-44a5-8a7c-22542a2f82e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f(g(x)) = 3x² + 1 at x=2.0: 13.0\n",
            "Chain rule: grad_f(g(x)) * grad_g(x) = 3 * 4.0 = 12.0\n",
            "Direct (6x): 12.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define functions\n",
        "def g(x): return x ** 2      # g(x) = x²\n",
        "def f(x): return 3 * x + 1   # f(x) = 3x + 1\n",
        "def fg(x): return f(g(x))    # f(g(x)) = 3x² + 1\n",
        "\n",
        "# Derivatives\n",
        "def grad_g(x): return 2 * x  # d/dx(x²) = 2x\n",
        "def grad_f(x): return 3      # d/dx(3x + 1) = 3\n",
        "\n",
        "# Chain rule: grad(fg, x) = grad(f, g(x)) * grad(g, x)\n",
        "x = 2.0\n",
        "chain_rule = grad_f(g(x)) * grad_g(x)\n",
        "direct = 6 * x  # d/dx(3x² + 1) = 6x\n",
        "\n",
        "print(f\"f(g(x)) = 3x² + 1 at x={x}: {fg(x)}\")\n",
        "print(f\"Chain rule: grad_f(g(x)) * grad_g(x) = {grad_f(g(x))} * {grad_g(x)} = {chain_rule}\")\n",
        "print(f\"Direct (6x): {direct}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVd4sV1VV_dX"
      },
      "source": [
        "### 4.8 Backpropagation with Computation Graphs\n",
        "\n",
        "A **computation graph** is a directed acyclic graph of operations.\n",
        "\n",
        "<img src=\"https://deeplearningwithpython.io/images/ch02/a_first_computation_graph.90dec1fc.png\" width=\"200\" alt=\"Computation graph representation of a two-layer neural network model\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9kY0EU9V_dX"
      },
      "source": [
        "**Simple example:**\n",
        "\n",
        "<img src=\"https://deeplearningwithpython.io/images/ch02/basic_computation_graph.f3e3c75a.png\" width=\"200\" alt=\"Basic computation graph with variables w, b, x, and loss calculation\">\n",
        "\n",
        "Variables: `w`, `b`, input `x`, target `y_true`\n",
        "- `x1 = x * w`\n",
        "- `x2 = x1 + b`\n",
        "- `loss = |y_true - x2|`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VS5-RMYIV_dX"
      },
      "source": [
        "**Forward Pass** (compute values top to bottom):\n",
        "\n",
        "<img src=\"https://deeplearningwithpython.io/images/ch02/basic_computation_graph_with_values.e15cd230.png\" width=\"250\" alt=\"Forward pass through computation graph showing computed values at each node\">\n",
        "\n",
        "With `x=2, w=3, b=1, y_true=4`:\n",
        "- `x1 = 2 * 3 = 6`\n",
        "- `x2 = 6 + 1 = 7`\n",
        "- `loss = |4 - 7| = 3`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OETEJaawV_dX"
      },
      "source": [
        "**Backward Pass** (compute gradients bottom to top):\n",
        "\n",
        "<img src=\"https://deeplearningwithpython.io/images/ch02/basic_computation_graph_backward.9e975200.png\" width=\"600\" alt=\"Backward pass through computation graph showing gradients at each edge\">\n",
        "\n",
        "For each edge: \"How much does output vary when input varies?\"\n",
        "- `grad(loss, x2) = -1` (since y_true < x2)\n",
        "- `grad(x2, x1) = 1`\n",
        "- `grad(x2, b) = 1`\n",
        "- `grad(x1, w) = x = 2`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8rLkEE0V_dY"
      },
      "source": [
        "**Chain Rule Application:**\n",
        "\n",
        "<img src=\"https://deeplearningwithpython.io/images/ch02/path_in_backward_graph.fe91e7d0.png\" width=\"600\" alt=\"Path from loss to w in the backward graph for computing gradients via chain rule\">\n",
        "\n",
        "```\n",
        "grad(loss, w) = grad(loss, x2) * grad(x2, x1) * grad(x1, w)\n",
        "              = -1 * 1 * 2 = -2\n",
        "\n",
        "grad(loss, b) = grad(loss, x2) * grad(x2, b)  \n",
        "              = -1 * 1 = -1\n",
        "```\n",
        "\n",
        "**This is backpropagation!** We \"back propagate\" loss contributions through the graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtXPmhl2V_dY",
        "outputId": "46a6b86e-842d-44cf-e711-63e01c168a34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forward Pass:\n",
            "x1 = x * w = 2.0 * 3.0 = 6.0\n",
            "x2 = x1 + b = 6.0 + 1.0 = 7.0\n",
            "loss = |y_true - x2| = |4.0 - 7.0| = 3.0\n",
            "\n",
            "Backward Pass (Chain Rule):\n",
            "grad(loss, w) = -1 * 1 * 2.0 = -2.0\n",
            "grad(loss, b) = -1 * 1 = -1\n",
            "\n",
            "After gradient step:\n",
            "w: 3.0 -> 3.2, b: 1.0 -> 1.1\n",
            "loss: 3.0 -> 3.5\n"
          ]
        }
      ],
      "source": [
        "# Manual backpropagation\n",
        "x = 2.0\n",
        "w = 3.0\n",
        "b = 1.0\n",
        "y_true = 4.0\n",
        "\n",
        "# Forward pass\n",
        "x1 = x * w          # 6\n",
        "x2 = x1 + b         # 7\n",
        "loss = abs(y_true - x2)  # 3\n",
        "\n",
        "print(\"Forward Pass:\")\n",
        "print(f\"x1 = x * w = {x} * {w} = {x1}\")\n",
        "print(f\"x2 = x1 + b = {x1} + {b} = {x2}\")\n",
        "print(f\"loss = |y_true - x2| = |{y_true} - {x2}| = {loss}\")\n",
        "\n",
        "# Backward pass\n",
        "grad_loss_x2 = -1 if y_true < x2 else 1  # -1\n",
        "grad_x2_x1 = 1\n",
        "grad_x2_b = 1\n",
        "grad_x1_w = x  # 2\n",
        "\n",
        "# Chain rule\n",
        "grad_loss_w = grad_loss_x2 * grad_x2_x1 * grad_x1_w\n",
        "grad_loss_b = grad_loss_x2 * grad_x2_b\n",
        "\n",
        "print(\"\\nBackward Pass (Chain Rule):\")\n",
        "print(f\"grad(loss, w) = {grad_loss_x2} * {grad_x2_x1} * {grad_x1_w} = {grad_loss_w}\")\n",
        "print(f\"grad(loss, b) = {grad_loss_x2} * {grad_x2_b} = {grad_loss_b}\")\n",
        "\n",
        "# Gradient descent step\n",
        "lr = 0.1\n",
        "w_new = w - lr * grad_loss_w\n",
        "b_new = b - lr * grad_loss_b\n",
        "loss_new = abs(y_true - (x * w_new + b_new))\n",
        "\n",
        "print(f\"\\nAfter gradient step:\")\n",
        "print(f\"w: {w} -> {w_new}, b: {b} -> {b_new}\")\n",
        "print(f\"loss: {loss} -> {loss_new}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yu7t37dcV_dY"
      },
      "source": [
        "### 4.9 Automatic Differentiation\n",
        "\n",
        "Modern frameworks compute gradients automatically. You write the forward pass; they handle gradients!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99SAkB7jV_dY",
        "outputId": "eb029fab-b3db-4d80-904e-bf41b6e433f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = 3.0\n",
            "y = x² = 9.0\n",
            "dy/dx = 2x = 6.0 (expected: 6.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# GradientTape records operations\n",
        "x = tf.Variable(3.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    y = x ** 2\n",
        "\n",
        "grad = tape.gradient(y, x)\n",
        "print(f\"x = {x.numpy()}\")\n",
        "print(f\"y = x² = {y.numpy()}\")\n",
        "print(f\"dy/dx = 2x = {grad.numpy()} (expected: {2 * x.numpy()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucsfcPMkV_dY",
        "outputId": "b84809d5-8f5b-42c0-e921-40f49633be0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_pred = 7.0, loss = 3.0\n",
            "grad_w = 2.0, grad_b = 1.0\n"
          ]
        }
      ],
      "source": [
        "# Multiple variables\n",
        "import tensorflow as tf\n",
        "\n",
        "w = tf.Variable(3.0)\n",
        "b = tf.Variable(1.0)\n",
        "x = tf.constant(2.0)\n",
        "y_true = tf.constant(4.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    y_pred = x * w + b\n",
        "    loss = tf.abs(y_true - y_pred)\n",
        "\n",
        "gradients = tape.gradient(loss, [w, b])\n",
        "print(f\"y_pred = {y_pred.numpy()}, loss = {loss.numpy()}\")\n",
        "print(f\"grad_w = {gradients[0].numpy()}, grad_b = {gradients[1].numpy()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmPH_JU3V_dY"
      },
      "source": [
        "---\n",
        "## 5. Looking Back at Our First Example\n",
        "\n",
        "<img src=\"https://deeplearningwithpython.io/images/ch02/deep-learning-in-3-figures-3_alt.40aa865d.png\" width=\"500\" alt=\"Deep learning overview - relationship between model, layers, loss function, and optimizer\">\n",
        "\n",
        "1. **Model**: Layers chained together, mapping inputs to predictions\n",
        "2. **Loss function**: Compares predictions to targets\n",
        "3. **Optimizer**: Uses gradients to update weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJvF_pVuV_dY"
      },
      "source": [
        "---\n",
        "## 6. Reimplementing from Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OlCIkiBV_dY"
      },
      "source": [
        "### 6.1 A Simple Dense Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "uGkhtCdtV_dY"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras import ops\n",
        "\n",
        "class NaiveDense:\n",
        "    def __init__(self, input_size, output_size, activation=None):\n",
        "        self.activation = activation\n",
        "        self.W = keras.Variable(shape=(input_size, output_size), initializer=\"uniform\")\n",
        "        self.b = keras.Variable(shape=(output_size,), initializer=\"zeros\")\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        x = ops.matmul(inputs, self.W) + self.b\n",
        "        if self.activation:\n",
        "            x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "    @property\n",
        "    def weights(self):\n",
        "        return [self.W, self.b]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvvkXLasV_dY"
      },
      "source": [
        "### 6.2 A Simple Sequential Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvG0rAViV_dY",
        "outputId": "00a4c6fc-2033-4397-fc4d-17b7f8da179b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model has 4 weight tensors\n"
          ]
        }
      ],
      "source": [
        "class NaiveSequential:\n",
        "    def __init__(self, layers):\n",
        "        self.layers = layers\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        x = inputs\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "    @property\n",
        "    def weights(self):\n",
        "        weights = []\n",
        "        for layer in self.layers:\n",
        "            weights += layer.weights\n",
        "        return weights\n",
        "\n",
        "# Create model\n",
        "model = NaiveSequential([\n",
        "    NaiveDense(28 * 28, 512, activation=ops.relu),\n",
        "    NaiveDense(512, 10, activation=ops.softmax),\n",
        "])\n",
        "print(f\"Model has {len(model.weights)} weight tensors\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1q8MqpwV_dY"
      },
      "source": [
        "### 6.3 Batch Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "OGYK46moV_dY"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "class BatchGenerator:\n",
        "    def __init__(self, images, labels, batch_size=128):\n",
        "        self.index = 0\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.num_batches = math.ceil(len(images) / batch_size)\n",
        "\n",
        "    def next(self):\n",
        "        images = self.images[self.index:self.index + self.batch_size]\n",
        "        labels = self.labels[self.index:self.index + self.batch_size]\n",
        "        self.index += self.batch_size\n",
        "        return images, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbQJaj4EV_dY"
      },
      "source": [
        "### 6.4 Training Step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "6SnZbo_KV_dZ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras import optimizers\n",
        "\n",
        "optimizer = optimizers.SGD(learning_rate=1e-3)\n",
        "\n",
        "def one_training_step(model, images_batch, labels_batch):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(images_batch)\n",
        "        per_sample_losses = ops.sparse_categorical_crossentropy(labels_batch, predictions)\n",
        "        average_loss = ops.mean(per_sample_losses)\n",
        "\n",
        "    gradients = tape.gradient(average_loss, model.weights)\n",
        "    optimizer.apply_gradients(zip(gradients, model.weights))\n",
        "    return average_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-oE2s5IV_dZ"
      },
      "source": [
        "### 6.5 Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "7D_wurnnV_dZ"
      },
      "outputs": [],
      "source": [
        "def fit(model, images, labels, epochs, batch_size=128):\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "        batch_gen = BatchGenerator(images, labels, batch_size)\n",
        "        for batch_idx in range(batch_gen.num_batches):\n",
        "            images_batch, labels_batch = batch_gen.next()\n",
        "            loss = one_training_step(model, images_batch, labels_batch)\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f\"  Batch {batch_idx}: loss = {float(loss):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQW-upUvV_dZ"
      },
      "source": [
        "### 6.6 Train and Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkUJeZ-5V_dZ",
        "outputId": "e0eb4fa2-3dd4-4805-f81a-6a606171e069"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "  Batch 0: loss = 2.3164\n",
            "  Batch 100: loss = 2.2866\n",
            "  Batch 200: loss = 2.2534\n",
            "  Batch 300: loss = 2.2167\n",
            "  Batch 400: loss = 2.1768\n",
            "Epoch 2/10\n",
            "  Batch 0: loss = 2.1355\n",
            "  Batch 100: loss = 2.1261\n",
            "  Batch 200: loss = 2.0769\n",
            "  Batch 300: loss = 2.0461\n",
            "  Batch 400: loss = 2.0079\n",
            "Epoch 3/10\n",
            "  Batch 0: loss = 1.9548\n",
            "  Batch 100: loss = 1.9638\n",
            "  Batch 200: loss = 1.8927\n",
            "  Batch 300: loss = 1.8648\n",
            "  Batch 400: loss = 1.8307\n",
            "Epoch 4/10\n",
            "  Batch 0: loss = 1.7614\n",
            "  Batch 100: loss = 1.7896\n",
            "  Batch 200: loss = 1.6949\n",
            "  Batch 300: loss = 1.6734\n",
            "  Batch 400: loss = 1.6483\n",
            "Epoch 5/10\n",
            "  Batch 0: loss = 1.5626\n",
            "  Batch 100: loss = 1.6084\n",
            "  Batch 200: loss = 1.4938\n",
            "  Batch 300: loss = 1.4833\n",
            "  Batch 400: loss = 1.4722\n",
            "Epoch 6/10\n",
            "  Batch 0: loss = 1.3735\n",
            "  Batch 100: loss = 1.4329\n",
            "  Batch 200: loss = 1.3044\n",
            "  Batch 300: loss = 1.3092\n",
            "  Batch 400: loss = 1.3140\n",
            "Epoch 7/10\n",
            "  Batch 0: loss = 1.2070\n",
            "  Batch 100: loss = 1.2741\n",
            "  Batch 200: loss = 1.1389\n",
            "  Batch 300: loss = 1.1601\n",
            "  Batch 400: loss = 1.1795\n",
            "Epoch 8/10\n",
            "  Batch 0: loss = 1.0684\n",
            "  Batch 100: loss = 1.1382\n",
            "  Batch 200: loss = 1.0018\n",
            "  Batch 300: loss = 1.0376\n",
            "  Batch 400: loss = 1.0693\n",
            "Epoch 9/10\n",
            "  Batch 0: loss = 0.9567\n",
            "  Batch 100: loss = 1.0250\n",
            "  Batch 200: loss = 0.8915\n",
            "  Batch 300: loss = 0.9388\n",
            "  Batch 400: loss = 0.9803\n",
            "Epoch 10/10\n",
            "  Batch 0: loss = 0.8677\n",
            "  Batch 100: loss = 0.9321\n",
            "  Batch 200: loss = 0.8036\n",
            "  Batch 300: loss = 0.8594\n",
            "  Batch 400: loss = 0.9085\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28 * 28)).astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28)).astype(\"float32\") / 255\n",
        "\n",
        "# Fresh model\n",
        "model = NaiveSequential([\n",
        "    NaiveDense(28 * 28, 512, activation=ops.relu),\n",
        "    NaiveDense(512, 10, activation=ops.softmax),\n",
        "])\n",
        "optimizer = optimizers.SGD(learning_rate=1e-3)\n",
        "\n",
        "fit(model, train_images, train_labels, epochs=10, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtBhb2d1V_dZ",
        "outputId": "89d02b66-7ce0-42ee-cd11-db80ffc39470"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test accuracy: 0.8361\n"
          ]
        }
      ],
      "source": [
        "# Evaluate\n",
        "predictions = model(test_images)\n",
        "predicted_labels = ops.argmax(predictions, axis=1)\n",
        "matches = predicted_labels == test_labels\n",
        "accuracy = float(ops.mean(ops.cast(matches, \"float32\")))\n",
        "print(f\"\\nTest accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEayk_bQV_dZ"
      },
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "### Tensors\n",
        "- Foundation of ML: defined by **dtype**, **rank**, **shape**\n",
        "- Manipulated via tensor operations (addition, matmul, element-wise)\n",
        "- Operations = geometric transformations\n",
        "\n",
        "### Neural Networks\n",
        "- Chains of tensor operations with **trainable weights**\n",
        "- Weights store learned knowledge\n",
        "- Activation functions enable nonlinearity\n",
        "\n",
        "### Derivatives & Gradients\n",
        "- **Derivative**: Slope at a point; how output changes with input\n",
        "- **Gradient**: Multi-dimensional derivative; vector of partial derivatives\n",
        "- **Key insight**: Move opposite to gradient to minimize\n",
        "\n",
        "### Training via Gradient Descent\n",
        "- **Learning** = minimizing loss by adjusting weights\n",
        "- **Mini-batch SGD**: Random batches, compute gradients, update weights\n",
        "- **Backpropagation**: Chain rule through computation graph\n",
        "- **Automatic differentiation**: Frameworks compute gradients for you\n",
        "\n",
        "### Key Components\n",
        "- **Loss function**: What to minimize\n",
        "- **Optimizer**: How to update weights (SGD, Adam, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I6-LACJXX1za"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}