{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNim3ZZs+iF1roI1o9o78g0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saffarizadeh/INSY5378/blob/main/OLS_and_Activation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deriving the OLS Estimator\n",
        "\n",
        "## Setup\n",
        "\n",
        "- **Model**: $y = X\\beta + \\varepsilon$\n",
        "- **Loss function** (Sum of Squared Errors): $L(\\beta) = (y - X\\beta)^T(y - X\\beta)$\n",
        "\n",
        "## Step 1: Expand the Loss Function\n",
        "\n",
        "Using the transpose rule $(A - B)^T = A^T - B^T$ and $(AB)^T = B^TA^T$:\n",
        "\n",
        "$$L(\\beta) = (y^T - \\beta^TX^T)(y - X\\beta)$$\n",
        "\n",
        "Expanding:\n",
        "\n",
        "$$L(\\beta) = y^Ty - y^TX\\beta - \\beta^TX^Ty + \\beta^TX^TX\\beta$$\n",
        "\n",
        "Since $y^TX\\beta$ is a scalar (1×n @ n×p @ p×1 = 1×1), it equals its transpose: $y^TX\\beta = \\beta^TX^Ty$\n",
        "\n",
        "$$L(\\beta) = y^Ty - 2\\beta^TX^Ty + \\beta^TX^TX\\beta$$\n",
        "\n",
        "## Step 2: Take the Partial Derivative\n",
        "\n",
        "Using matrix calculus rules:\n",
        "- $\\frac{\\partial}{\\partial \\beta}(\\beta^Tc) = c$ for constant vector $c$\n",
        "- $\\frac{\\partial}{\\partial \\beta}(\\beta^TA\\beta) = 2A\\beta$ for symmetric matrix $A$\n",
        "\n",
        "Applying to each term:\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial \\beta} = 0 - 2X^Ty + 2X^TX\\beta$$\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial \\beta} = -2X^Ty + 2X^TX\\beta$$\n",
        "\n",
        "## Step 3: Solve for $\\hat{\\beta}$\n",
        "\n",
        "Set the derivative equal to zero:\n",
        "\n",
        "$$-2X^Ty + 2X^TX\\beta = 0$$\n",
        "\n",
        "$$X^TX\\beta = X^Ty$$\n",
        "\n",
        "$$\\boxed{\\hat{\\beta} = (X^TX)^{-1}X^Ty}$$\n",
        "\n",
        "This is the **OLS closed-form solution**."
      ],
      "metadata": {
        "id": "nzoSHeXQsrh5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data (Synthetically generated so that we know the true coefficients)"
      ],
      "metadata": {
        "id": "ditkcQFFteZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Set seed for reproducibility\n",
        "np.random.seed(1)\n",
        "\n",
        "# Basic OLS\n",
        "n = 1000\n",
        "x1 = np.linspace(-5, 5, n)\n",
        "x2 = np.random.normal(5, 2, n)\n",
        "b = np.ones(n)\n",
        "x = np.column_stack((b, x1, x2))\n",
        "\n",
        "beta = np.array([[1.5, 2.4, -1.2]]).T  # intercept, x1, x2\n",
        "y = x @ beta + np.random.normal(0, 1, (n, 1))"
      ],
      "metadata": {
        "id": "wCaL397VIWAD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Estimating the coefficients"
      ],
      "metadata": {
        "id": "LsCUxk1nto_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "beta_hat = np.linalg.inv(x.T @ x) @ x.T @ y\n",
        "\n",
        "print(\"True beta:\", beta.T)\n",
        "print(\"Estimated beta:\", beta_hat.T)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ydj1FNFbtoxC",
        "outputId": "5dd459c4-b995-445e-90ff-9f4e23f0b04e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True beta: [[ 1.5  2.4 -1.2]]\n",
            "Estimated beta: [[ 1.46998721  2.39310086 -1.18870767]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example of activation functions"
      ],
      "metadata": {
        "id": "Pp38RkTBtxaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding some activation after the fact - In reality, the estimation approach is different but the concept is similar\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "y_hat = x @ beta_hat\n",
        "y_hat_sig = sigmoid(y_hat)\n",
        "\n",
        "print(f\"\\ny_hat range: [{y_hat.min():.3f}, {y_hat.max():.3f}]\")\n",
        "print(f\"y_hat_sig range: [{y_hat_sig.min():.3f}, {y_hat_sig.max():.3f}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xIorNFzLW3O",
        "outputId": "7bc93979-64cd-42f2-af00-5684d4ac81de"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "y_hat range: [-20.443, 13.275]\n",
            "y_hat_sig range: [0.000, 1.000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7hs-bEhWMEMq"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}