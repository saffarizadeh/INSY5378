{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saffarizadeh/INSY5378/blob/main/Math_Foundations_Prep_Exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SDBC05_tnby"
      },
      "source": [
        "<img src=\"https://kambizsaffari.com/Logo/College_of_Business.cmyk-hz-lg.png\" width=\"500px\"/>\n",
        "\n",
        "# *INSY 5378 - Advanced AI*\n",
        "\n",
        "# **Prep Exercises: The Mathematical Building Blocks of Neural Networks**\n",
        "\n",
        "Instructor: Dr. Kambiz Saffari\n",
        "\n",
        "---\n",
        "\n",
        "# Prep Exercises  Chapter 2: The Mathematical Building Blocks of Neural Networks\n",
        "\n",
        "This notebook prepares you for Chapter 2 of *Deep Learning with Python*.\n",
        "\n",
        "**You already know Python and NumPy basics.** Here we focus on **hands-on drills** that will make the chapter much easier to follow:\n",
        "\n",
        "- Understanding **tensors**\n",
        "- Working with **shapes**, **ndim**, and **dtype**\n",
        "- **Slicing** and **batching** data\n",
        "- **Element-wise operations** and the **ReLU activation**\n",
        "- **Broadcasting** (how NumPy handles different-shaped arrays)\n",
        "- **Matrix multiplication** (the core operation in neural networks)\n",
        "- **Reshaping** and **flattening** data\n",
        "- A taste of **gradient descent**\n",
        "\n",
        "---\n",
        "## How to Use This Notebook\n",
        "\n",
        "1. Run cells from top to bottom\n",
        "2. Fill in the `# YOUR CODE HERE` or `None` parts\n",
        "3. Most exercises have **`assert` statements** that verify your answer - if no error appears, you got it right! (you see: ✅)\n",
        "4. If stuck, read the hints and use `print()` to inspect shapes\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRGECaXEtnbz"
      },
      "source": [
        "## Setup\n",
        "Run this cell first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQx1_0Mftnbz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "def softmax(x, axis=-1):\n",
        "    \"\"\"Numerically stable softmax.\"\"\"\n",
        "    x = x - np.max(x, axis=axis, keepdims=True)\n",
        "    exp_x = np.exp(x)\n",
        "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
        "\n",
        "print(\"Setup complete! ✅\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNPcuR1ftnbz"
      },
      "source": [
        "---\n",
        "## Part 1: From NumPy Arrays to Tensors\n",
        "\n",
        "### What is a Tensor?\n",
        "\n",
        "In deep learning, you'll constantly hear the word **tensor**. Here's the good news: **you already know what tensors are!**\n",
        "\n",
        "A tensor is simply a container for numerical data - exactly what NumPy arrays are. Deep learning frameworks like TensorFlow and PyTorch just use different terminology:\n",
        "\n",
        "| NumPy Term | Deep Learning Term | Dimensions |\n",
        "|------------|-------------------|------------|\n",
        "| 0-D array | Scalar | 0 |\n",
        "| 1-D array | Vector | 1 |\n",
        "| 2-D array | Matrix | 2 |\n",
        "| n-D array | n-D Tensor | n |\n",
        "\n",
        "The **rank** of a tensor = number of dimensions = `ndim` in NumPy.\n",
        "\n",
        "### Key Attributes (You Already Know These!)\n",
        "\n",
        "- **`ndim`**: Number of axes (dimensions)\n",
        "- **`shape`**: Size along each axis, e.g., `(3, 4)` means 3 rows, 4 columns\n",
        "- **`dtype`**: Data type (`float32`, `int64`, `uint8`, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fmjo--htnb0"
      },
      "outputs": [],
      "source": [
        "# Examples of tensors with different ranks\n",
        "\n",
        "scalar = np.array(42)                    # Rank 0 (scalar)\n",
        "vector = np.array([1, 2, 3, 4, 5])       # Rank 1 (vector)\n",
        "matrix = np.array([[1, 2], [3, 4]])      # Rank 2 (matrix)\n",
        "tensor_3d = np.ones((2, 3, 4))           # Rank 3 (3D tensor)\n",
        "\n",
        "print(f\"Scalar:    ndim={scalar.ndim}, shape={scalar.shape}\")\n",
        "print(f\"Vector:    ndim={vector.ndim}, shape={vector.shape}\")\n",
        "print(f\"Matrix:    ndim={matrix.ndim}, shape={matrix.shape}\")\n",
        "print(f\"3D Tensor: ndim={tensor_3d.ndim}, shape={tensor_3d.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3HNI4zAtnb0"
      },
      "source": [
        "### Exercise 1.1: Create Tensors with Specific Properties\n",
        "\n",
        "Create arrays matching the requirements. All `assert` statements should pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhxJTeNatnb0"
      },
      "outputs": [],
      "source": [
        "# 1A) Create a scalar (0-D) integer array with value 12\n",
        "x0 = None  # YOUR CODE HERE\n",
        "\n",
        "# 1B) Create a 1-D float32 array: [12, 3, 6, 14, 7]\n",
        "x1 = None  # YOUR CODE HERE\n",
        "\n",
        "# 1C) Create a 2-D integer matrix with shape (3, 5):\n",
        "# [[5, 78, 2, 34, 0],\n",
        "#  [6, 79, 3, 35, 1],\n",
        "#  [7, 80, 4, 36, 2]]\n",
        "x2 = None  # YOUR CODE HERE\n",
        "\n",
        "# 1D) Create a 3-D tensor of zeros with shape (2, 3, 4)\n",
        "x3 = None  # YOUR CODE HERE\n",
        "\n",
        "# --- Self-checks ---\n",
        "assert isinstance(x0, np.ndarray) and x0.ndim == 0 and x0.item() == 12\n",
        "assert x1.shape == (5,) and x1.dtype == np.float32\n",
        "assert x2.shape == (3, 5) and x2.ndim == 2\n",
        "assert x3.shape == (2, 3, 4) and x3.ndim == 3\n",
        "\n",
        "print(\"✅ Exercise 1.1 passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtrLdFY2tnb0"
      },
      "source": [
        "### Real-World Data as Tensors\n",
        "\n",
        "Different types of data have standard tensor shapes:\n",
        "\n",
        "| Data Type | Shape | Example |\n",
        "|-----------|-------|--------|\n",
        "| Tabular | `(samples, features)` | 1000 people × 5 attributes |\n",
        "| Grayscale images | `(samples, height, width)` | 100 images × 28 × 28 |\n",
        "| Color images | `(samples, height, width, channels)` | 100 images × 64 × 64 × 3 |\n",
        "| Time series | `(samples, timesteps, features)` | 250 days × 390 minutes × 3 values |\n",
        "\n",
        "**Note:** The first axis is almost always the **samples** (or **batch**) axis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCAxTU3ftnb0"
      },
      "source": [
        "### Exercise 1.2: Interpret Tensor Shapes\n",
        "\n",
        "What shape should each dataset have?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mW-UJhtstnb0"
      },
      "outputs": [],
      "source": [
        "# a) 1000 people, each with 5 features (age, income, education, children, credit_score)\n",
        "people_shape = None  # YOUR ANSWER AS A TUPLE\n",
        "\n",
        "# b) 32 grayscale images, each 28×28 pixels\n",
        "grayscale_shape = None  # YOUR ANSWER\n",
        "\n",
        "# c) 16 color (RGB) images, each 64×64 pixels\n",
        "color_shape = None  # YOUR ANSWER\n",
        "\n",
        "# d) Stock data: 250 days, 390 minutes per day, 3 values per minute (price, high, low)\n",
        "stock_shape = None  # YOUR ANSWER\n",
        "\n",
        "# --- Self-checks ---\n",
        "assert people_shape == (1000, 5)\n",
        "assert grayscale_shape == (32, 28, 28)\n",
        "assert color_shape == (16, 64, 64, 3)\n",
        "assert stock_shape == (250, 390, 3)\n",
        "\n",
        "print(\"✅ Exercise 1.2 passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSpyQURAtnb0"
      },
      "source": [
        "---\n",
        "## Part 2: Reshaping and Preprocessing Data\n",
        "\n",
        "### Why Reshape?\n",
        "\n",
        "Neural networks expect data in specific shapes. Common operations:\n",
        "\n",
        "1. **Flattening images**: Dense layers need 1-D input, but images are 2-D. We reshape `(28, 28)` → `(784,)`\n",
        "2. **Normalizing**: Scale pixel values from `[0, 255]` to `[0, 1]`\n",
        "3. **Type conversion**: Convert from `uint8` to `float32`\n",
        "\n",
        "This is exactly what you'll see in Chapter 2's MNIST preprocessing!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqSuGMIxtnb0"
      },
      "source": [
        "### Exercise 2.1: Preprocess Image Data\n",
        "\n",
        "Given images with shape `(n, 28, 28)` and dtype `uint8` (values 0-255):\n",
        "1. Reshape to `(n, 784)` — flatten each image\n",
        "2. Convert to `float32`\n",
        "3. Normalize to `[0, 1]` by dividing by 255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPm3kzAftnb0"
      },
      "outputs": [],
      "source": [
        "n = 256\n",
        "images = np.random.randint(0, 256, size=(n, 28, 28), dtype=np.uint8)\n",
        "print(f\"Original: shape={images.shape}, dtype={images.dtype}\")\n",
        "\n",
        "# YOUR CODE: reshape, convert dtype, normalize\n",
        "flat = None\n",
        "\n",
        "# --- Self-checks ---\n",
        "assert flat.shape == (n, 784), f\"Expected shape (256, 784), got {flat.shape}\"\n",
        "assert flat.dtype == np.float32, f\"Expected float32, got {flat.dtype}\"\n",
        "assert 0.0 <= flat.min() and flat.max() <= 1.0, \"Values should be in [0, 1]\"\n",
        "\n",
        "print(f\"Processed: shape={flat.shape}, dtype={flat.dtype}, range=[{flat.min():.2f}, {flat.max():.2f}]\")\n",
        "print(\"✅ Exercise 2.1 passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glcpWycRtnb1"
      },
      "source": [
        "**Hints:**\n",
        "- `images.reshape(n, 784)` or `images.reshape(n, -1)`\n",
        "- `.astype(np.float32)` for type conversion\n",
        "- Divide by 255.0 (not 255) to ensure float division"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL_Rq71-tnb1"
      },
      "source": [
        "### Exercise 2.2: Basic Reshaping Practice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZaA11JFtnb1"
      },
      "outputs": [],
      "source": [
        "arr = np.arange(12)  # [0, 1, 2, ..., 11]\n",
        "print(f\"Original: {arr}, shape={arr.shape}\")\n",
        "\n",
        "# a) Reshape to (3, 4)\n",
        "a = None  # YOUR CODE HERE\n",
        "\n",
        "# b) Reshape to (4, 3)\n",
        "b = None  # YOUR CODE HERE\n",
        "\n",
        "# c) Reshape to (2, 2, 3)\n",
        "c = None  # YOUR CODE HERE\n",
        "\n",
        "# --- Self-checks ---\n",
        "assert a.shape == (3, 4)\n",
        "assert b.shape == (4, 3)\n",
        "assert c.shape == (2, 2, 3)\n",
        "\n",
        "print(f\"(3,4):\\n{a}\\n\")\n",
        "print(f\"(4,3):\\n{b}\\n\")\n",
        "print(f\"(2,2,3):\\n{c}\")\n",
        "print(\"\\n✅ Exercise 2.2 passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKsuCfvgtnb1"
      },
      "source": [
        "---\n",
        "## Part 3: Slicing and Batching\n",
        "\n",
        "### Why Batching Matters\n",
        "\n",
        "Neural networks don't process one sample at a time — they process **batches** (groups of samples). This is more efficient and helps with training stability.\n",
        "\n",
        "If `batch_size = 128` and we want batch `n`:\n",
        "- Start index: `n * batch_size`\n",
        "- End index: `(n + 1) * batch_size`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTJOGOAwtnb1"
      },
      "source": [
        "### Exercise 3.1: Extract Batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SscJe9K_tnb1"
      },
      "outputs": [],
      "source": [
        "def get_batch(data, batch_size, n):\n",
        "    \"\"\"Return batch number n (0-indexed) from data.\"\"\"\n",
        "    # YOUR CODE: compute start and end, return the slice\n",
        "    return None\n",
        "\n",
        "# Test data\n",
        "data = np.arange(60000)  # Like MNIST's 60000 training samples\n",
        "\n",
        "b0 = get_batch(data, 128, 0)  # First batch\n",
        "b3 = get_batch(data, 128, 3)  # Fourth batch\n",
        "\n",
        "# --- Self-checks ---\n",
        "assert b0.shape == (128,) and b0[0] == 0 and b0[-1] == 127\n",
        "assert b3[0] == 384 and b3[-1] == 511  # 128*3=384, 128*4-1=511\n",
        "\n",
        "print(f\"Batch 0: indices {b0[0]} to {b0[-1]}\")\n",
        "print(f\"Batch 3: indices {b3[0]} to {b3[-1]}\")\n",
        "print(\"✅ Exercise 3.1 passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld-SuO3Rtnb1"
      },
      "source": [
        "### Exercise 3.2: Slice Image Regions (Center Crop)\n",
        "\n",
        "A common operation is cropping images. Extract the **center 14×14 region** from 28×28 images.\n",
        "\n",
        "For a 28×28 image, the center 14×14 uses rows `7:21` and columns `7:21`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ulr7ppi-tnb1"
      },
      "outputs": [],
      "source": [
        "images = np.random.randint(0, 256, size=(64, 28, 28), dtype=np.uint8)\n",
        "\n",
        "# YOUR CODE: extract center 14×14 from all 64 images\n",
        "center = None\n",
        "\n",
        "# --- Self-checks ---\n",
        "assert center.shape == (64, 14, 14)\n",
        "assert np.array_equal(center, images[:, 7:21, 7:21])\n",
        "\n",
        "print(f\"Original shape: {images.shape}\")\n",
        "print(f\"Center crop shape: {center.shape}\")\n",
        "print(\"✅ Exercise 3.2 passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPiBn6X3tnb1"
      },
      "source": [
        "---\n",
        "## Part 4: Element-wise Operations and ReLU\n",
        "\n",
        "### What Are Element-wise Operations?\n",
        "\n",
        "Operations applied to each element independently:\n",
        "```python\n",
        "[1, 2, 3] + [10, 20, 30] = [11, 22, 33]  # Add corresponding elements\n",
        "[1, 2, 3] * 2 = [2, 4, 6]                 # Multiply each by 2\n",
        "```\n",
        "\n",
        "### The ReLU Activation Function\n",
        "\n",
        "ReLU (Rectified Linear Unit) is the most common activation function:\n",
        "\n",
        "$$\\text{ReLU}(x) = \\max(x, 0)$$\n",
        "\n",
        "- Positive values stay the same\n",
        "- Negative values become 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWGCODz0tnb1"
      },
      "outputs": [],
      "source": [
        "# Visualize ReLU\n",
        "x = np.linspace(-5, 5, 100)\n",
        "y = np.maximum(x, 0)  # This is ReLU!\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(x, y, 'b-', linewidth=2)\n",
        "plt.axhline(0, color='k', linewidth=0.5)\n",
        "plt.axvline(0, color='k', linewidth=0.5)\n",
        "plt.xlabel('Input'); plt.ylabel('Output')\n",
        "plt.title('ReLU: max(x, 0)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE1yym0ltnb1"
      },
      "source": [
        "### Exercise 4.1: Implement ReLU (Vectorized vs Loop)\n",
        "\n",
        "NumPy's **vectorized** operations are much faster than Python loops. Implement ReLU both ways to see the difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXnRLxextnb1"
      },
      "outputs": [],
      "source": [
        "def relu_loop(x):\n",
        "    \"\"\"ReLU using Python loops (slow).\"\"\"\n",
        "    out = x.copy()\n",
        "    # YOUR CODE: nested loops to set negative values to 0\n",
        "    for i in range(out.shape[0]):\n",
        "        for j in range(out.shape[1]):\n",
        "            pass  # YOUR CODE HERE\n",
        "    return out\n",
        "\n",
        "def relu_vectorized(x):\n",
        "    \"\"\"ReLU using NumPy (fast).\"\"\"\n",
        "    # YOUR CODE: use np.maximum\n",
        "    return None\n",
        "\n",
        "# Test\n",
        "x = np.random.randn(200, 300).astype(np.float32)\n",
        "a = relu_loop(x)\n",
        "b = relu_vectorized(x)\n",
        "\n",
        "assert np.allclose(a, b), \"Results don't match!\"\n",
        "\n",
        "# Timing comparison\n",
        "t0 = time.time(); _ = relu_vectorized(x); t_vec = time.time() - t0\n",
        "t0 = time.time(); _ = relu_loop(x); t_loop = time.time() - t0\n",
        "\n",
        "print(f\"Vectorized: {t_vec:.4f}s | Loop: {t_loop:.4f}s | Speedup: {t_loop/t_vec:.0f}x\")\n",
        "print(\"✅ Exercise 4.1 passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JV9dpYYWtnb2"
      },
      "source": [
        "---\n",
        "## Part 5: Broadcasting\n",
        "\n",
        "### What is Broadcasting?\n",
        "\n",
        "Broadcasting lets NumPy operate on arrays with different shapes by automatically \"stretching\" the smaller one.\n",
        "\n",
        "**Example:** Adding a bias vector to a batch of outputs:\n",
        "```python\n",
        "output = np.zeros((32, 10))  # 32 samples, 10 features\n",
        "bias = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])  # 10 biases\n",
        "result = output + bias  # bias is added to EACH row!\n",
        "```\n",
        "\n",
        "### Broadcasting Rules\n",
        "NumPy compares shapes from right to left. Dimensions are compatible if:\n",
        "- They're equal, OR\n",
        "- One of them is 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "letYVp9Stnb2"
      },
      "source": [
        "### Exercise 5.1: Broadcasting in Action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eA4J5t-Vtnb2"
      },
      "outputs": [],
      "source": [
        "X = np.random.random((32, 10)).astype(np.float32)  # 32 samples, 10 features\n",
        "y = np.random.random((10,)).astype(np.float32)     # bias vector\n",
        "\n",
        "# Method 1: Broadcasting (automatic)\n",
        "Z_broadcast = None  # YOUR CODE: X + y\n",
        "\n",
        "# Method 2: Explicit tiling (manual — to understand what broadcasting does)\n",
        "Y_tiled = None  # YOUR CODE: tile y to shape (32, 10) using np.tile(y, (32, 1))\n",
        "Z_tiled = None  # YOUR CODE: X + Y_tiled\n",
        "\n",
        "# --- Self-checks ---\n",
        "assert Z_broadcast.shape == (32, 10)\n",
        "assert Y_tiled.shape == (32, 10)\n",
        "assert np.allclose(Z_broadcast, Z_tiled)\n",
        "\n",
        "print(\"Broadcasting adds y to each row of X automatically!\")\n",
        "print(\"✅ Exercise 5.1 passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ArFMroxtnb2"
      },
      "source": [
        "### Exercise 5.2: Center Data (Subtract Mean)\n",
        "\n",
        "A common preprocessing step: subtract the mean of each feature to center the data around 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTxgkXwItnb2"
      },
      "outputs": [],
      "source": [
        "# Data: 5 samples, 3 features with different scales\n",
        "data = np.array([[10, 100, 1000],\n",
        "                 [20, 200, 2000],\n",
        "                 [30, 300, 3000],\n",
        "                 [40, 400, 4000],\n",
        "                 [50, 500, 5000]], dtype=np.float32)\n",
        "\n",
        "# Step 1: Compute mean of each feature (column) using axis=0\n",
        "feature_means = None  # YOUR CODE HERE\n",
        "\n",
        "# Step 2: Subtract means using broadcasting\n",
        "centered = None  # YOUR CODE HERE\n",
        "\n",
        "# --- Self-checks ---\n",
        "assert feature_means.shape == (3,)\n",
        "assert np.allclose(feature_means, [30, 300, 3000])\n",
        "assert np.allclose(centered.mean(axis=0), [0, 0, 0], atol=1e-5)\n",
        "\n",
        "print(f\"Feature means: {feature_means}\")\n",
        "print(f\"Centered data mean per feature: {centered.mean(axis=0)}\")\n",
        "print(\"✅ Exercise 5.2 passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b65BaSgjtnb2"
      },
      "source": [
        "---\n",
        "## Part 6: Matrix Multiplication (Dot Product)\n",
        "\n",
        "### The Core Operation in Neural Networks\n",
        "\n",
        "Every layer in a neural network uses matrix multiplication:\n",
        "```python\n",
        "output = input @ weights + bias\n",
        "```\n",
        "\n",
        "### Matrix Multiplication Rules\n",
        "\n",
        "For `A @ B` to work:\n",
        "- A has shape `(m, n)`\n",
        "- B has shape `(n, p)` — **inner dimensions must match!**\n",
        "- Result has shape `(m, p)`\n",
        "\n",
        "### Don't Confuse These!\n",
        "- `A * B` = element-wise multiplication\n",
        "- `A @ B` = matrix multiplication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_h88fLx-tnb2"
      },
      "outputs": [],
      "source": [
        "# Demonstrate the difference\n",
        "A = np.array([[1, 2], [3, 4]])\n",
        "B = np.array([[5, 6], [7, 8]])\n",
        "\n",
        "print(\"A:\")\n",
        "print(A)\n",
        "print(\"\\nB:\")\n",
        "print(B)\n",
        "print(\"\\nElement-wise (A * B):\")\n",
        "print(A * B)\n",
        "print(\"\\nMatrix multiplication (A @ B):\")\n",
        "print(A @ B)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWLGqnfotnb2"
      },
      "source": [
        "### Exercise 6.1: Matrix Multiplication Practice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBybzh5Jtnb2"
      },
      "outputs": [],
      "source": [
        "v = np.random.random((32,)).astype(np.float32)   # vector\n",
        "w = np.random.random((32,)).astype(np.float32)   # vector\n",
        "A = np.random.random((20, 32)).astype(np.float32) # matrix\n",
        "B = np.random.random((32, 15)).astype(np.float32) # matrix\n",
        "\n",
        "# YOUR CODE:\n",
        "# 1. Vector dot product: v · w → scalar\n",
        "s = None\n",
        "\n",
        "# 2. Matrix-vector product: A @ v → vector of shape (20,)\n",
        "Av = None\n",
        "\n",
        "# 3. Matrix-matrix product: A @ B → matrix of shape (20, 15)\n",
        "AB = None\n",
        "\n",
        "# --- Self-checks ---\n",
        "assert np.isscalar(s) or (isinstance(s, np.ndarray) and s.shape == ())\n",
        "assert Av.shape == (20,)\n",
        "assert AB.shape == (20, 15)\n",
        "\n",
        "print(f\"v · w = scalar\")\n",
        "print(f\"A(20,32) @ v(32,) = shape {Av.shape}\")\n",
        "print(f\"A(20,32) @ B(32,15) = shape {AB.shape}\")\n",
        "print(\"✅ Exercise 6.1 passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2XbeH-5tnb2"
      },
      "source": [
        "### Exercise 6.2: Implement a Dense Layer Forward Pass\n",
        "\n",
        "A Dense layer computes: `y = X @ W + b`\n",
        "\n",
        "Where:\n",
        "- `X`: input batch with shape `(batch_size, input_dim)`\n",
        "- `W`: weights with shape `(input_dim, output_dim)`\n",
        "- `b`: bias with shape `(output_dim,)`\n",
        "- `y`: output with shape `(batch_size, output_dim)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehvdIkpAtnb2"
      },
      "outputs": [],
      "source": [
        "def dense_forward(X, W, b):\n",
        "    \"\"\"Compute forward pass of a Dense layer.\"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    return None\n",
        "\n",
        "# Test\n",
        "batch, in_dim, out_dim = 64, 20, 10\n",
        "X = np.random.randn(batch, in_dim).astype(np.float32)\n",
        "W = np.random.randn(in_dim, out_dim).astype(np.float32)\n",
        "b = np.random.randn(out_dim).astype(np.float32)\n",
        "\n",
        "y = dense_forward(X, W, b)\n",
        "\n",
        "# --- Self-checks ---\n",
        "assert y.shape == (batch, out_dim)\n",
        "assert np.allclose(y, X @ W + b)\n",
        "\n",
        "print(f\"Input: {X.shape} → Output: {y.shape}\")\n",
        "print(\"✅ Exercise 6.2 passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEurrmiTtnb2"
      },
      "source": [
        "---\n",
        "## Part 7: Softmax and Cross-Entropy Loss\n",
        "\n",
        "### Classification Output\n",
        "\n",
        "For classification, neural networks output **logits** (raw scores). We convert these to probabilities using **softmax**:\n",
        "\n",
        "$$\\text{softmax}(z)_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$$\n",
        "\n",
        "The **cross-entropy loss** measures how wrong the predictions are:\n",
        "\n",
        "$$\\text{loss} = -\\log(p_{\\text{true class}})$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hVyiijZtnb2"
      },
      "source": [
        "### Exercise 7.1: Softmax and Cross-Entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EODQTDzstnb2"
      },
      "outputs": [],
      "source": [
        "K = 10  # 10 classes (like digits 0-9)\n",
        "z = np.random.randn(K).astype(np.float32)  # logits (raw scores)\n",
        "y_true = 7  # true class\n",
        "\n",
        "# YOUR CODE:\n",
        "# 1. Apply softmax to get probabilities (use the softmax function from setup)\n",
        "p = None\n",
        "\n",
        "# 2. Compute cross-entropy loss: -log(probability of true class)\n",
        "loss = None\n",
        "\n",
        "# --- Self-checks ---\n",
        "assert p.shape == (K,)\n",
        "assert np.allclose(p.sum(), 1.0, atol=1e-6), \"Probabilities should sum to 1\"\n",
        "assert loss >= 0, \"Loss should be non-negative\"\n",
        "\n",
        "print(f\"Logits z: {z[:5]}... (showing first 5)\")\n",
        "print(f\"Probabilities p: {p[:5]}... (showing first 5)\")\n",
        "print(f\"Sum of probabilities: {p.sum():.6f}\")\n",
        "print(f\"Probability of true class {y_true}: {p[y_true]:.4f}\")\n",
        "print(f\"Cross-entropy loss: {loss:.4f}\")\n",
        "print(\"✅ Exercise 7.1 passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pjt9tLxKtnb7"
      },
      "source": [
        "---\n",
        "## Part 8: A Taste of Gradient Descent\n",
        "\n",
        "### How Neural Networks Learn\n",
        "\n",
        "Training a neural network involves:\n",
        "1. Compute predictions (forward pass)\n",
        "2. Compute loss (how wrong are we?)\n",
        "3. Compute gradients (which direction reduces loss?)\n",
        "4. Update weights: `w = w - learning_rate * gradient`\n",
        "\n",
        "Let's implement this for a simple 1D linear model: `y_pred = w*x + b`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGuDWOr1tnb7"
      },
      "source": [
        "### Exercise 8.1: Numerical Gradient Descent\n",
        "\n",
        "We'll fit a line to data using **numerical gradients** (approximating derivatives with small differences).\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial w} \\approx \\frac{L(w + \\epsilon) - L(w - \\epsilon)}{2\\epsilon}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQhjygiQtnb7"
      },
      "outputs": [],
      "source": [
        "# Generate data: y ≈ 3x + 2 with some noise\n",
        "x = np.linspace(-1, 1, 200).astype(np.float32)\n",
        "y = (3.0 * x + 2.0 + 0.1 * np.random.randn(*x.shape)).astype(np.float32)\n",
        "\n",
        "plt.scatter(x, y, alpha=0.5, s=10)\n",
        "plt.xlabel('x'); plt.ylabel('y')\n",
        "plt.title('Data: y ≈ 3x + 2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ci3cZFqHtnb7"
      },
      "outputs": [],
      "source": [
        "def mse_loss(w, b, x, y):\n",
        "    \"\"\"Mean squared error: average of (y_pred - y)^2\"\"\"\n",
        "    # YOUR CODE: compute y_pred = w*x + b, then MSE\n",
        "    return None\n",
        "\n",
        "def numerical_gradients(w, b, x, y, eps=1e-3):\n",
        "    \"\"\"Compute gradients using finite differences.\"\"\"\n",
        "    # dL/dw ≈ (L(w+eps, b) - L(w-eps, b)) / (2*eps)\n",
        "    dw = None  # YOUR CODE\n",
        "\n",
        "    # dL/db ≈ (L(w, b+eps) - L(w, b-eps)) / (2*eps)\n",
        "    db = None  # YOUR CODE\n",
        "\n",
        "    return dw, db\n",
        "\n",
        "# Initialize weights\n",
        "w, b = 0.0, 0.0\n",
        "lr = 0.1  # learning rate\n",
        "\n",
        "# Compute initial loss\n",
        "loss_before = mse_loss(w, b, x, y)\n",
        "\n",
        "# Compute gradients\n",
        "dw, db = numerical_gradients(w, b, x, y)\n",
        "\n",
        "# Update weights (one step of gradient descent)\n",
        "w_new = w - lr * dw\n",
        "b_new = b - lr * db\n",
        "\n",
        "# Compute new loss\n",
        "loss_after = mse_loss(w_new, b_new, x, y)\n",
        "\n",
        "print(f\"Before: w={w:.3f}, b={b:.3f}, loss={loss_before:.4f}\")\n",
        "print(f\"Gradients: dw={dw:.4f}, db={db:.4f}\")\n",
        "print(f\"After:  w={w_new:.3f}, b={b_new:.3f}, loss={loss_after:.4f}\")\n",
        "\n",
        "# --- Self-check ---\n",
        "assert loss_after < loss_before, \"Loss should decrease after one gradient step!\"\n",
        "print(\"\\n✅ Exercise 8.1 passed! Loss decreased.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxByzOD5tnb7"
      },
      "source": [
        "### Optional: Train for Many Steps\n",
        "\n",
        "If you finish early, try training for 100 steps to see w approach 3 and b approach 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJfyBkHbtnb7"
      },
      "outputs": [],
      "source": [
        "# OPTIONAL: Full training loop\n",
        "w, b = 0.0, 0.0\n",
        "lr = 0.1\n",
        "losses = []\n",
        "\n",
        "for step in range(100):\n",
        "    loss = mse_loss(w, b, x, y)\n",
        "    losses.append(loss)\n",
        "    dw, db = numerical_gradients(w, b, x, y)\n",
        "    w = w - lr * dw\n",
        "    b = b - lr * db\n",
        "\n",
        "    if step % 20 == 0:\n",
        "        print(f\"Step {step}: w={w:.3f}, b={b:.3f}, loss={loss:.4f}\")\n",
        "\n",
        "print(f\"\\nFinal: w={w:.3f} (target: 3.0), b={b:.3f} (target: 2.0)\")\n",
        "\n",
        "# Plot loss curve\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Step'); plt.ylabel('Loss')\n",
        "plt.title('Training Loss Over Time')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRQb6wCRtnb7"
      },
      "source": [
        "---\n",
        "## Part 9: Full Forward Pass Simulation\n",
        "\n",
        "Let's put everything together: preprocessing → Dense layer 1 → ReLU → Dense layer 2 → Softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQs84Ia4tnb7"
      },
      "outputs": [],
      "source": [
        "# Simulate a mini neural network for digit classification\n",
        "np.random.seed(42)\n",
        "\n",
        "# 1. GENERATE DATA: 32 random 8×8 \"images\"\n",
        "batch_size = 32\n",
        "raw_images = np.random.randint(0, 256, size=(batch_size, 8, 8), dtype=np.uint8)\n",
        "print(f\"Raw images: {raw_images.shape}, dtype={raw_images.dtype}\")\n",
        "\n",
        "# 2. PREPROCESS: flatten, convert to float32, normalize\n",
        "X = raw_images.reshape(batch_size, -1).astype(np.float32) / 255.0\n",
        "print(f\"Preprocessed: {X.shape}, dtype={X.dtype}, range=[{X.min():.2f}, {X.max():.2f}]\")\n",
        "\n",
        "# 3. LAYER 1: Dense (64 → 32) + ReLU\n",
        "W1 = np.random.randn(64, 32).astype(np.float32) * 0.1\n",
        "b1 = np.zeros(32, dtype=np.float32)\n",
        "z1 = X @ W1 + b1\n",
        "a1 = np.maximum(z1, 0)  # ReLU\n",
        "print(f\"After Layer 1 + ReLU: {a1.shape}\")\n",
        "\n",
        "# 4. LAYER 2: Dense (32 → 10)\n",
        "W2 = np.random.randn(32, 10).astype(np.float32) * 0.1\n",
        "b2 = np.zeros(10, dtype=np.float32)\n",
        "z2 = a1 @ W2 + b2\n",
        "print(f\"After Layer 2: {z2.shape}\")\n",
        "\n",
        "# 5. SOFTMAX: convert to probabilities\n",
        "probs = softmax(z2)\n",
        "print(f\"Probabilities: {probs.shape}, each row sums to {probs[0].sum():.4f}\")\n",
        "\n",
        "# 6. PREDICTIONS\n",
        "predictions = np.argmax(probs, axis=1)\n",
        "print(f\"\\nPredicted classes for first 10 samples: {predictions[:10]}\")\n",
        "\n",
        "print(\"\\n✅ Complete forward pass simulation done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0haP01ztnb8"
      },
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "You've practiced all the key operations used in Chapter 2:\n",
        "\n",
        "| Concept | What You Did | Used In Chapter For |\n",
        "|---------|-------------|--------------------|\n",
        "| Tensors | Created arrays of different ranks | Storing data & weights |\n",
        "| Reshaping | Flattened images (28×28 → 784) | Preprocessing |\n",
        "| Slicing | Extracted batches and crops | Training loop |\n",
        "| Element-wise ops | Implemented ReLU | Activation functions |\n",
        "| Broadcasting | Added bias to batches | Dense layer computation |\n",
        "| Matrix multiplication | Implemented Dense layer | Core layer operation |\n",
        "| Softmax | Converted logits to probabilities | Classification output |\n",
        "| Gradient descent | Minimized loss with gradients | Training the network |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2D0WcobyvvTn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}