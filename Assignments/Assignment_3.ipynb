{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saffarizadeh/INSY5378/blob/main/Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header_cell"
      },
      "source": [
        "<img src=\"https://kambizsaffari.com/Logo/College_of_Business.cmyk-hz-lg.png\" width=\"500px\"/>\n",
        "\n",
        "# *INSY 5378*\n",
        "\n",
        "# **Assignment: Introduction to TensorFlow, PyTorch, JAX, and Keras**\n",
        "\n",
        "Instructor: Dr. Kambiz Saffari\n",
        "\n",
        "---"
      ],
      "id": "header_cell"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructions_cell"
      },
      "source": [
        "**Instructions**\n",
        "- This assignment covers concepts from **Chapter 3: Introduction to TensorFlow, PyTorch, JAX, and Keras**.\n",
        "- Write your answers directly in the provided cells.\n",
        "- You may add additional cells if you want to test ideas, but only answers in the marked cells will be graded.\n"
      ],
      "id": "instructions_cell"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1_markdown"
      },
      "source": [
        "## Question 1: Tensors, Variables, and Gradients Across Frameworks\n",
        "\n",
        "The chapter shows that TensorFlow, PyTorch, and JAX each handle tensors and gradient computation differently. This question gives you hands-on experience with each one.\n",
        "\n",
        "**Part A - TensorFlow: Variables and GradientTape**\n",
        "\n",
        "1. Create a TensorFlow constant tensor `b` from the list `[[1, 2, 3], [4, 5, 6]]` with dtype `float32`. Try to assign `b[0, 0] = 99.0`. In a comment, explain why this fails.\n",
        "2. Create a `tf.Variable` called `v` initialized with the same values. Use `v[0, 0].assign(99.0)` to modify it. Print `v`.\n",
        "3. Create a `tf.Variable` with value `3.0`. Inside a `tf.GradientTape` scope, compute `result = tf.square(input_var)`. Retrieve and print the gradient. In a comment, verify the result analytically.\n",
        "\n",
        "**Part B - PyTorch: Autograd and Gradient Accumulation**\n",
        "\n",
        "1. Create `input_var = torch.tensor(3.0, requires_grad=True)`. Compute `result = torch.square(input_var)`, call `result.backward()`, and print `input_var.grad`.\n",
        "2. **Without resetting** the gradient, compute and call `backward()` again. Print `input_var.grad`. In a comment, explain why the value doubled and why `model.zero_grad()` matters during training.\n",
        "\n",
        "**Part C - JAX: Statelessness and Functional Gradients**\n",
        "\n",
        "1. Create `x = jnp.array([1, 2, 3], dtype=\"float32\")`. Try `x[0] = 10.0` (it will fail). Use `x.at[0].set(10.0)` to create `new_x`. Print both. In a comment, explain why JAX forbids in-place modification.\n",
        "2. Define `compute_loss(x)` returning `jnp.square(x)`. Use `jax.value_and_grad(compute_loss)` to get the loss and gradient for `jnp.array(3.0)`. Print both.\n",
        "\n",
        "In a comment at the end, briefly describe one key strength and one key weakness of each framework (TensorFlow, PyTorch, JAX) as discussed in the chapter.\n"
      ],
      "id": "q1_markdown"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1_code"
      },
      "outputs": [],
      "source": [
        "# Write your answer to Question 1 here"
      ],
      "id": "q1_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2_markdown"
      },
      "source": [
        "## Question 2: The Keras Layer Class\n",
        "\n",
        "The chapter describes the `Layer` as the central abstraction in Keras. A layer encapsulates weights (state) and a forward-pass computation.\n",
        "\n",
        "**Part A - Implement a Custom Layer**\n",
        "\n",
        "Following the `SimpleDense` example from the chapter, implement a custom Keras layer by subclassing `keras.Layer`:\n",
        "\n",
        "- `__init__(self, units, activation=None)` should store `units` and `activation`.\n",
        "- `build(self, input_shape)` should create weights `self.W` and `self.b` using `self.add_weight()`.\n",
        "- `call(self, inputs)` should compute `y = keras.ops.matmul(inputs, self.W) + self.b` and apply the activation if provided.\n",
        "\n",
        "Instantiate your layer with `units=32` and `activation=keras.ops.relu`, pass a test input of shape `(2, 784)` through it, and print the output shape.\n",
        "\n",
        "**Part B - Understanding the Design**\n",
        "\n",
        "In a comment, answer:\n",
        "1. Why does Keras create weights in `build()` rather than `__init__()`? What is \"automatic shape inference\"?\n",
        "2. Why do we put our computation in `call()` instead of `__call__()`?\n"
      ],
      "id": "q2_markdown"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2_code"
      },
      "outputs": [],
      "source": [
        "# Write your answer to Question 2 here"
      ],
      "id": "q2_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3_markdown"
      },
      "source": [
        "## Question 3: Compile, Fit, and the Keras Workflow\n",
        "\n",
        "Before training a model you must configure the learning process with `compile()`. This question explores those choices.\n",
        "\n",
        "**Part A - Compile a Model Two Ways**\n",
        "\n",
        "Create a simple model:\n",
        "```python\n",
        "model = keras.Sequential([keras.layers.Dense(1)])\n",
        "```\n",
        "\n",
        "1. Compile it using **string shortcuts**: optimizer `\"rmsprop\"`, loss `\"mean_squared_error\"`, metrics `[\"accuracy\"]`.\n",
        "2. Compile the same model again using **object instances** instead, and this time pass a custom `learning_rate` of `0.01` to the optimizer. Print the model's optimizer learning rate to confirm it is set correctly.\n",
        "\n",
        "In a comment, explain when you would prefer using object instances over string shortcuts.\n",
        "\n",
        "**Part B - Choosing the Right Loss Function**\n",
        "\n",
        "The chapter emphasizes that picking the right loss function is critical.\n",
        "\n",
        "For each scenario below, write a one-line `model.compile()` call with the appropriate loss (you can reuse the model above or create new ones). You do not need to train these models.\n",
        "\n",
        "1. Binary classification (two classes, single output with sigmoid activation).\n",
        "2. Multi-class classification with integer labels (e.g., MNIST with labels 0-9).\n",
        "3. Regression (predicting a continuous value).\n",
        "\n",
        "In a comment, explain: what are the three things `compile()` configures, and why does the chapter say the loss function is the most important one to get right?\n"
      ],
      "id": "q3_markdown"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3_code"
      },
      "outputs": [],
      "source": [
        "# Write your answer to Question 3 here"
      ],
      "id": "q3_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4_markdown"
      },
      "source": [
        "## Question 4: End-to-End Keras Model with Validation\n",
        "\n",
        "This question walks through the complete Keras workflow on a binary classification task.\n",
        "\n",
        "**Part A - Generate Data and Create a Validation Split**\n",
        "\n",
        "Create a linearly separable dataset (as shown in the chapter):\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "num_samples_per_class = 1000\n",
        "negative_samples = np.random.multivariate_normal(\n",
        "    mean=[0, 3], cov=[[1, 0.5], [0.5, 1]], size=num_samples_per_class\n",
        ")\n",
        "positive_samples = np.random.multivariate_normal(\n",
        "    mean=[3, 0], cov=[[1, 0.5], [0.5, 1]], size=num_samples_per_class\n",
        ")\n",
        "inputs = np.vstack((negative_samples, positive_samples)).astype(np.float32)\n",
        "targets = np.vstack((\n",
        "    np.zeros((num_samples_per_class, 1), dtype=\"float32\"),\n",
        "    np.ones((num_samples_per_class, 1), dtype=\"float32\"),\n",
        "))\n",
        "```\n",
        "\n",
        "Shuffle the data and split it into 70% training and 30% validation using `np.random.permutation` (as shown in the chapter). Print the shapes of your training and validation sets.\n",
        "\n",
        "In a comment, explain why we must keep training and validation data strictly separate.\n",
        "\n",
        "**Part B - Build, Compile, Train**\n",
        "\n",
        "```python\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "])\n",
        "```\n",
        "\n",
        "Compile with optimizer `\"rmsprop\"`, loss `\"binary_crossentropy\"`, and metrics `[\"binary_accuracy\"]`. Train for 15 epochs with `batch_size=16`, passing the validation data. Store the return value of `fit()` in `history`.\n",
        "\n",
        "**Part C - Evaluate, Predict, and Interpret**\n",
        "\n",
        "1. Use `model.evaluate()` on the validation set and print the loss and accuracy.\n",
        "2. Use `model.predict()` on the first 5 validation inputs. For each, print the predicted probability, the predicted class (threshold 0.5), and the actual label.\n",
        "3. Print the training loss and validation loss from the last epoch using `history.history`.\n",
        "\n",
        "In a comment, answer:\n",
        "1. What is the difference between `model(inputs)` and `model.predict(inputs)`?\n",
        "2. This model is a single Dense layer (a linear classifier). The chapter says choosing a topology defines a \"hypothesis space.\" What kind of decision boundary can this model learn, and what would you change to handle data that is not linearly separable?\n"
      ],
      "id": "q4_markdown"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4_code"
      },
      "outputs": [],
      "source": [
        "# Write your answer to Question 4 here"
      ],
      "id": "q4_code"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "submission_reminder"
      },
      "source": [
        "---\n",
        "**Submission Reminder**\n",
        "- Make sure your notebook runs from top to bottom without errors.\n",
        "- Clearly label all answers.\n",
        "- Include all required comments and explanations. They are part of your grade.\n"
      ],
      "id": "submission_reminder"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "empty_cell"
      },
      "id": "empty_cell",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
